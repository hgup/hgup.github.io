{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"ML/","title":"Machine Learning","text":"<p>Machine learning content here. I started reading the book G\u00e9ron, Aur\u00e9lien. \u201cHands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow\u201d and found it extremely useful.</p> <p>Firstly I put down some basics one needs to grasp to get comfortable with ML</p> <ol> <li>Getting used to using packages and knowing the norms and paradigm of using it.</li> <li>Knowing where to look for a certain feature in Scikit Learn.</li> <li>Being able to build custom classes in Scikit Learn</li> </ol> <p>I learnt these on the way while going through the book and created my own notebooks while following along in the Hands On section.</p>","tags":["Machine Learning"]},{"location":"ML/custom-classes-in-sklearn/","title":"Custom Classes in sklearn","text":"<p>We are going to use the housing dataset from this.</p> <pre><code>housing.info()\n</code></pre> <pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 16512 entries, 12655 to 19773\nData columns (total 13 columns):\n #   Column                    Non-Null Count  Dtype   \n---  ------                    --------------  -----   \n 0   longitude                 16512 non-null  float64 \n 1   latitude                  16512 non-null  float64 \n 2   housing_median_age        16512 non-null  float64 \n 3   total_rooms               16512 non-null  float64 \n 4   total_bedrooms            16512 non-null  float64 \n 5   population                16512 non-null  float64 \n 6   households                16512 non-null  float64 \n 7   median_income             16512 non-null  float64 \n 8   ocean_proximity           16512 non-null  object  \n 9   income_cat                16512 non-null  category\n 10  rooms_per_household       16512 non-null  float64 \n 11  bedrooms_per_room         16354 non-null  float64 \n 12  population_per_household  16512 non-null  float64 \ndtypes: category(1), float64(11), object(1)\nmemory usage: 1.7+ MB\n</code></pre> <p>Now, we utilize <code>sklearn.base</code>'s <code>BaseEstimator</code> and <code>TransformerMixin</code> to make a custom transformer:</p> <ul> <li><code>BaseEstimator</code> will give us a nice constructor class so that we don't need take care of all the base <code>*args</code> and <code>**kwargs</code> </li> <li><code>TransformerMixin</code> will give us the <code>.fit_transform()</code> function for free since we have have to define the <code>.fit()</code> and <code>.transform()</code> on our own. This was mentioned in Sklearn Design Consistency section.</li> </ul> <p>Here is what the class looks like, we are trying to add the following attributes to our DF, so we are writing a transformation:</p> <pre><code>from sklearn.base import BaseEstimator, TransformerMixin\n\nrooms_ix, bedrooms_ix, population_ix, households_ix  = 3, 4, 5, 6\n\nclass CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n    def __init__(self, add_bedrooms_per_room = True):\n        self.add_bedrooms_per_room = add_bedrooms_per_room\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        rooms_per_household = X[:, rooms_ix] / X[:, households_ix]\n        population_per_household = X[:, population_ix] / X[:, households_ix]\n        if self.add_bedrooms_per_room:\n            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n            return np.c_[X, rooms_per_household, population_per_household, bedrooms_per_room] # (1)\n        else:\n            return np.c_[X, rooms_per_household, population_per_household] #(1)\n\n\nattr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)\nhousing_extra_attribs = attr_adder.transform(housing.values)\n</code></pre> <ol> <li><code>np.c_</code> is for concatenating arrays as columns. <code>np.c_[X, rooms_per_household, population_per_household]</code> will concatenate the already 2d <code>X</code> with the two columns <code>rooms_per_household</code> and <code>population_per_household</code>. If <code>X</code> was 1000x10 then the final output would be 1000x12 (the two new columns added)</li> </ol> <p>Should practice this pattern multiple times to get comfortable and so that thinking about it becomes natural.</p>","tags":["Machine Learning","Technique"]},{"location":"ML/packages/","title":"Package Notes","text":"","tags":["Machine Learning"]},{"location":"ML/packages/#pandas-tricks","title":"<code>pandas</code> tricks","text":"<ul> <li><code>pd.cut()</code> helps you cut a series. There are two ways to do so<ul> <li>By specifying how many categories we need like so <code>pd.cut(dataFrame, NO_OF_CATEGORIES)</code></li> <li>By specifying the exact bins of categories like so <code>pd.cut(dataFrame, bins=[0., 1.5, 3.0, 4.5, np.inf], labels= [1,2,3,4,5])</code></li> <li>Labelling is done to give a name (in this case ordinal) to each of the categories</li> </ul> </li> </ul>","tags":["Machine Learning"]},{"location":"ML/packages/#pddataframe","title":"<code>pd.DataFrame</code>","text":"<ul> <li> <p>#concept axis refers to the row, column or a higher dimensional point (e.g. <code>axis=0</code> refers to rows, <code>axis=1</code> refers to columns. <code>axis=3</code> can refer to time, if we have data across years and each row corresponds to the same entity.)</p> </li> <li> <p><code>loc</code> and <code>iloc</code> have this commonly known distinction that the first is used to index by label but the latter by <code>index</code>.</p> </li> <li>But also know that when we take a subset of the dataframe (say, main) (lets say while test-train splitting of the data), the original <code>index</code> values of main df is preserved as the labels of the subsets.</li> <li>e.g. <code>main = [1,2,3,4,5,6,7]</code> are the default labels (for an unlabelled df main), and upon splitting it we get subsets <code>A = [1,3,4,5]</code> and <code>B = [2,6,7]</code></li> <li>So now note that we cannot do <code>A.loc[range(1,5)]</code> since we don't have label 2 in our <code>A</code>. But <code>A.iloc[range(1,5)]</code> will definitely work since indices are always in sequence.</li> </ul>","tags":["Machine Learning"]},{"location":"ML/packages/#tricks","title":"Tricks","text":"<ul> <li><code>list(df)</code> gives us a list of all the columns of the <code>pd.DataFrame</code>. Instead of using <code>df.columns</code> which gives us an <code>pd.Index</code> of column names.</li> </ul>","tags":["Machine Learning"]},{"location":"ML/packages/#pdseries","title":"<code>pd.Series</code>","text":"<ul> <li> <p><code>pd.Series.apply()</code> applies a function to each element of the series and returns another series with the output of each of those function call in the same order.</p> </li> <li> <p>if you want the \"label\" to each of the items in a pandas series, its actually called <code>index</code></p> </li> </ul>","tags":["Machine Learning"]},{"location":"ML/packages/#pdplotting","title":"<code>pd.plotting</code>","text":"<pre><code>housing.plot(\n    kind=\"scatter\",x='longitude',y='latitude',\n    alpha=.4, #(2)\n    s=housing['population']/100, #(1)\n    label=\"population\",\n    c=\"median_house_value\", #(3)\n    colorbar=True,\n    cmap=plt.get_cmap('jet') #(4)\n)\n</code></pre> <ol> <li><code>s</code> is the radius of the circle.</li> <li><code>alpha</code> opacity.</li> <li><code>c</code> is the color (always give the name of the property (e.g. <code>\"population\"</code> instead of <code>housing['population']</code> whenever possible, here we needed to make an adjustment to the size).</li> <li><code>cmap</code> gives us the color map.</li> </ol> <p><code>scatter_matrix</code> is used to plot correlation maps.</p>","tags":["Machine Learning"]},{"location":"ML/packages/#numpy-tricks","title":"<code>numpy</code> tricks","text":"","tags":["Machine Learning"]},{"location":"ML/packages/#npndarray","title":"<code>np.ndarray</code>","text":"<ul> <li>N-dimensional array</li> <li>Use <code>iloc</code> to reference by index</li> <li>Use <code>loc</code> to reference by label (==check this==: It is also possible to pass in an index, if label for the data is not available)</li> </ul>","tags":["Machine Learning"]},{"location":"ML/packages/#intuition","title":"Intuition","text":"","tags":["Machine Learning"]},{"location":"ML/packages/#indices","title":"Indices","text":"<ul> <li>What does <code>X[:,:]</code> mean?</li> <li>How did you say all rows? How did you say all columns?</li> <li>So, what is <code>X[a:b,:]</code>, (where <code>a</code> and <code>b</code> are integers)?<ul> <li>Rows/Records are lower dimensional, so the first index  (here <code>a:b</code>) always refer to rows. This is what we call <code>axis=0</code></li> <li>Columns/Headers/Attributes are higher dimensional, so the second index (here <code>:</code>) always refer to column. This is what we call <code>axis=1</code></li> <li>Thus, <code>X[a:b,:]</code> means, \"select all columns from the records whose indices start from <code>a</code> and do not equal or exceed <code>b</code>\"</li> <li>Similarly, <code>X[:,:c]</code> means \"select those columns with index \\(&lt;\\) <code>c</code> from all records\"</li> </ul> </li> </ul>","tags":["Machine Learning"]},{"location":"ML/packages/#general-tricks","title":"General Tricks","text":"","tags":["Machine Learning"]},{"location":"ML/packages/#typecasting","title":"Typecasting","text":"<ul> <li><code>pandas.core.series.Series</code>.<code>astype(np.int8)</code> to convert string to integers (or float to integers)</li> </ul>","tags":["Machine Learning"]},{"location":"ML/sklearn-notes/","title":"Scikit Learn Notes","text":"","tags":["Machine Learning"]},{"location":"ML/sklearn-notes/#sklearn-legend","title":"<code>sklearn</code> Legend","text":"import task <code>sklearn.model_selection</code> Helps in selecting between models.Also, splitting train-test <code>sklearn.impute</code> Missing Values <code>sklearn.preprocessing</code> Change the form of data.Ordinal Categorical \\(\\to\\) Numbers. Make it machine friendly <code>sklearn.base</code> Access base classes to create custom <code>estimator</code>, <code>transformer</code> <code>sklearn.pipeline</code> Create pipelines <code>sklearn.compose</code> Access to column transformers(take care of categorical and numerical columns in the same transformation) <code>sklearn.metrics</code> Score functions; Performance metrics; Distance computations <code>sklearn.tree</code> Decision tree based models <code>sklearn.ensemble</code> Various ensemble methods (e.g. <code>RandomForestRegressor</code>) <code>sklearn.svm</code> Dedicated for Support Vector Machines??? <code>sklearn.multiclass</code> Access to <code>OneVsRestClassifier</code> and <code>OneVsOneClassifier</code> to override default multiclass behavior in Binary classifiers <code>sklearn.neighbors</code> Distance based models","tags":["Machine Learning"]},{"location":"ML/sklearn-notes/#design-philosophy","title":"Design Philosophy","text":"","tags":["Machine Learning"]},{"location":"ML/sklearn-notes/#consistency","title":"Consistency","text":"<p>All objects share a consistent and simple interface</p> Object Explanation Example Estimators - Any object that can estimate some parameters based on the estimator- The estimation is done by the <code>.fit()</code> method.- Takes only a dataset as a parameter (or 2 (data and labels) for supervised learning algorithms)- Any other parameter is considered as a <code>hyperparameter</code> (e.g. strategy) Imputer Transformers - Any object that can transform the dataset.- The transformation is performed by the <code>.transform()</code> method, which takes input as a dataset to be transformed- Both <code>.fit()</code> and <code>.transform()</code> can be conveniently be called together (with possible optimization) via the <code>.fit_transform()</code> function Imputer Predictors - Capable of making predictions.- The <code>.predict()</code> method takes a dataset of new instances and returns a dataset of corresponding predictions.- A predictor also has a <code>.score()</code> method that measures the quality of predictions, given a test set (w/ labels, in case of supervised learning). <p>Having learnt the above, now try implementing an [[Imputing Data using Scikit-Learn|imputing strategy in Scikit-Learn]].</p>","tags":["Machine Learning"]},{"location":"ML/sklearn-notes/#inspection","title":"Inspection","text":"<ol> <li>Estimator's hyperparameters are accessible directly via public instance variables, e.g. <code>imputer.strategy</code></li> <li>Estimator's learned parameters are accessible in a similar fashion but with an <code>_</code> (underscore) suffix, e.g. <code>imputer.statistics_</code></li> </ol>","tags":["Machine Learning"]},{"location":"ML/sklearn-notes/#nonproliferation-of-classes","title":"Nonproliferation of Classes","text":"<ol> <li>All outputs given in the form of <code>numpy</code> arrays  or <code>scipy</code> sparse matrices.</li> <li>Hyperparameters are regular python <code>string</code> or <code>numbers</code></li> </ol>","tags":["Machine Learning"]},{"location":"ML/sklearn-notes/#composition","title":"Composition","text":"<p>Existing building blocks 1. Can be reused 2. Can be combined to create a <code>Pipeline</code> (arbitrary sequence of transformations followed by a final estimator)</p>","tags":["Machine Learning"]},{"location":"ML/sklearn-notes/#sensible-defaults","title":"Sensible defaults","text":"<ol> <li>Most common defaults</li> <li>Baseline system is quick to create</li> </ol>","tags":["Machine Learning"]},{"location":"ML/sklearn-notes/#models","title":"Models","text":"","tags":["Machine Learning"]},{"location":"ML/sklearn-notes/#classifiers","title":"Classifiers","text":"Model <code>sgd</code> Stochastic Gradient Descent uses <code>.decision_function()</code> Multiple <code>forest</code> Random Forest Classifier uses <code>.predict_proba()</code> Multiple Naive Bayes Multiple <code>svc</code> Support Vector Machine Classifier uses <code>.decision_function()</code> Binary w/ OvO strategy","tags":["Machine Learning"]},{"location":"ML/sklearn-notes/#attributes","title":"Attributes","text":"","tags":["Machine Learning"]},{"location":"ML/handson/","title":"Hands-on Machine Learning","text":"<ol> <li>Classification</li> </ol>","tags":["Machine Learning"]},{"location":"ML/handson/03/","title":"Classification","text":"In\u00a0[1]: Copied! <pre>from sklearn.datasets import fetch_openml\n\nmnist = fetch_openml('mnist_784', version=1, cache=True)\nmnist.keys()\n</pre> from sklearn.datasets import fetch_openml  mnist = fetch_openml('mnist_784', version=1, cache=True) mnist.keys() Out[1]: <pre>dict_keys(['data', 'target', 'frame', 'categories', 'feature_names', 'target_names', 'DESCR', 'details', 'url'])</pre> In\u00a0[2]: Copied! <pre>X = mnist.data\ny = mnist.target\n</pre> X = mnist.data y = mnist.target In\u00a0[3]: Copied! <pre>type(mnist.target[0])\n</pre> type(mnist.target[0]) Out[3]: <pre>str</pre> In\u00a0[4]: Copied! <pre>X.shape # 28 x 28 px = 784\n</pre> X.shape # 28 x 28 px = 784 Out[4]: <pre>(70000, 784)</pre> In\u00a0[5]: Copied! <pre>X.head()\n</pre> X.head() Out[5]: pixel1 pixel2 pixel3 pixel4 pixel5 pixel6 pixel7 pixel8 pixel9 pixel10 ... pixel775 pixel776 pixel777 pixel778 pixel779 pixel780 pixel781 pixel782 pixel783 pixel784 0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 2 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 3 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 4 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 <p>5 rows \u00d7 784 columns</p> In\u00a0[6]: Copied! <pre>y.shape\n</pre> y.shape Out[6]: <pre>(70000,)</pre> <p>We will graph an instance's feature vector. An instance here is an image of a digit stored as a row in the dataset.</p> In\u00a0[7]: Copied! <pre>import matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\nINDEX = 0\n\nsome_digit = np.array(X.iloc[INDEX])\nsome_digit_image = some_digit.reshape(28,28)\n\nplt.imshow(some_digit_image,cmap=\"binary\")\nplt.axis('off')\nplt.show()\n</pre> import matplotlib as mpl import matplotlib.pyplot as plt import pandas as pd import numpy as np  INDEX = 0  some_digit = np.array(X.iloc[INDEX]) some_digit_image = some_digit.reshape(28,28)  plt.imshow(some_digit_image,cmap=\"binary\") plt.axis('off') plt.show() In\u00a0[8]: Copied! <pre>y = y.astype(np.int8)\n</pre> y = y.astype(np.int8) In\u00a0[9]: Copied! <pre>X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]\n</pre> X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:] In\u00a0[10]: Copied! <pre>y_train_5 = (y_train == 5)\ny_test_5 = (y_test == 5)\nprint(y_train_5)\n</pre> y_train_5 = (y_train == 5) y_test_5 = (y_test == 5) print(y_train_5) <pre>0         True\n1        False\n2        False\n3        False\n4        False\n         ...  \n59995    False\n59996    False\n59997     True\n59998    False\n59999    False\nName: class, Length: 60000, dtype: bool\n</pre> In\u00a0[11]: Copied! <pre>from sklearn.linear_model import SGDClassifier\nsgd_clf = SGDClassifier(random_state=42)\nsgd_clf.fit(X_train, y_train_5)\n</pre> from sklearn.linear_model import SGDClassifier sgd_clf = SGDClassifier(random_state=42) sgd_clf.fit(X_train, y_train_5) Out[11]: <pre>SGDClassifier(random_state=42)</pre> In\u00a0[\u00a0]: Copied! <pre>sgd_clf.predict([some_digit])\n</pre> sgd_clf.predict([some_digit]) In\u00a0[13]: Copied! <pre>y_train_pred_original = sgd_clf.predict(X_train)\n</pre> y_train_pred_original = sgd_clf.predict(X_train) In\u00a0[14]: Copied! <pre>from sklearn.model_selection import cross_val_predict\n\n# get a clean prediction in each iteration\ny_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv = 3)\n</pre> from sklearn.model_selection import cross_val_predict  # get a clean prediction in each iteration y_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv = 3) In\u00a0[15]: Copied! <pre># Implementation of Cross Validation\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.base import clone\n\nskfolds = StratifiedKFold(n_splits=3, random_state=42, shuffle=True)\n\nfor train_index, test_index in skfolds.split(X_train, y_train_5):\n    # creates a new, untrained model using the same hyperparameters as the original\n    clone_clf = clone(sgd_clf)\n    X_train_folds = X_train.iloc[train_index]\n    y_train_folds = y_train_5.iloc[train_index]\n    X_test_folds = X_train.iloc[test_index]\n    y_test_folds = y_train_5.iloc[test_index]\n\n    clone_clf.fit(X_train_folds, y_train_folds)\n    y_pred = clone_clf.predict(X_test_folds)\n    n_correct = sum(y_pred == y_test_folds)\n    print(n_correct / len(y_pred))\n</pre> # Implementation of Cross Validation  from sklearn.model_selection import StratifiedKFold from sklearn.base import clone  skfolds = StratifiedKFold(n_splits=3, random_state=42, shuffle=True)  for train_index, test_index in skfolds.split(X_train, y_train_5):     # creates a new, untrained model using the same hyperparameters as the original     clone_clf = clone(sgd_clf)     X_train_folds = X_train.iloc[train_index]     y_train_folds = y_train_5.iloc[train_index]     X_test_folds = X_train.iloc[test_index]     y_test_folds = y_train_5.iloc[test_index]      clone_clf.fit(X_train_folds, y_train_folds)     y_pred = clone_clf.predict(X_test_folds)     n_correct = sum(y_pred == y_test_folds)     print(n_correct / len(y_pred))   <pre>0.9669\n0.91625\n0.96785\n</pre> In\u00a0[16]: Copied! <pre>from sklearn.metrics import confusion_matrix\ny_train_perfect_predictions = y_train_5\nconfusion_matrix(y_train_5, y_train_perfect_predictions)\n</pre> from sklearn.metrics import confusion_matrix y_train_perfect_predictions = y_train_5 confusion_matrix(y_train_5, y_train_perfect_predictions) Out[16]: <pre>array([[54579,     0],\n       [    0,  5421]], dtype=int64)</pre> In\u00a0[17]: Copied! <pre>from sklearn.metrics import precision_score, recall_score\n\nprec = precision_score(y_train_5, y_train_pred)\n</pre> from sklearn.metrics import precision_score, recall_score  prec = precision_score(y_train_5, y_train_pred) In\u00a0[18]: Copied! <pre>rec = recall_score(y_train_5, y_train_pred)\n</pre> rec = recall_score(y_train_5, y_train_pred) In\u00a0[19]: Copied! <pre>f1 = 2/(1/prec + 1/rec)\nf1\n</pre> f1 = 2/(1/prec + 1/rec) f1 Out[19]: <pre>0.7325171197343847</pre> In\u00a0[20]: Copied! <pre>from sklearn.metrics import f1_score\n\nf1_score(y_train_5,y_train_pred)\n</pre> from sklearn.metrics import f1_score  f1_score(y_train_5,y_train_pred) Out[20]: <pre>0.7325171197343846</pre> <p>Stochastic Gradient Descent has a decision function that gives a score to an X value. Based on a threshold, it classifies as positive or negative.</p> <p><code>sklearn</code> doesn't let us set the threshold value. But we have access to the decision score to make predictions. We can call the <code>decision_function()</code></p> In\u00a0[21]: Copied! <pre>sgd_clf.decision_function([some_digit])\n</pre> sgd_clf.decision_function([some_digit]) <pre>c:\\Users\\hursh\\miniconda3\\envs\\tf2\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but SGDClassifier was fitted with feature names\n  warnings.warn(\n</pre> Out[21]: <pre>array([2164.22030239])</pre> <p>How to decide which threshold to use?</p> In\u00a0[22]: Copied! <pre>y_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3, method=\"decision_function\")\n</pre> y_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3, method=\"decision_function\") In\u00a0[23]: Copied! <pre>from sklearn.metrics import precision_recall_curve\n\nprecisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)\n</pre> from sklearn.metrics import precision_recall_curve  precisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores) In\u00a0[24]: Copied! <pre>thresholds\n</pre> thresholds Out[24]: <pre>array([-106527.45300471, -105763.22240074, -105406.2965229 , ...,\n         38871.26391927,   42216.05562787,   49441.43765905])</pre> In\u00a0[25]: Copied! <pre>def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")\n    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\")\n    plt.legend(loc=\"center right\", fontsize=16)\n    plt.xlabel(\"Threshold\", fontsize=16)\n    plt.grid(True)\n    plt.axis([-50000, 50000, 0, 1])\n\nplt.figure(figsize=(8,4))\nplot_precision_recall_vs_threshold(precisions, recalls, thresholds)\nrecall_90_precision = recalls[np.argmax(precisions &gt; 0.9)]\nthresholds_90_precision = thresholds[np.argmax(precisions &gt; 0.9)]\nplt.plot([-50000, thresholds_90_precision],[0.9, 0.9], 'r:')\nplt.plot([thresholds_90_precision, thresholds_90_precision],[0.9,0.], 'r:')\nplt.plot([-50000, thresholds_90_precision],[recall_90_precision,recall_90_precision], 'r:')\nplt.plot([thresholds_90_precision],[0.9],'ro')\nplt.plot([thresholds_90_precision],[recall_90_precision],'ro')\nplt.show()\n</pre>  def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):     plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")     plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\")     plt.legend(loc=\"center right\", fontsize=16)     plt.xlabel(\"Threshold\", fontsize=16)     plt.grid(True)     plt.axis([-50000, 50000, 0, 1])  plt.figure(figsize=(8,4)) plot_precision_recall_vs_threshold(precisions, recalls, thresholds) recall_90_precision = recalls[np.argmax(precisions &gt; 0.9)] thresholds_90_precision = thresholds[np.argmax(precisions &gt; 0.9)] plt.plot([-50000, thresholds_90_precision],[0.9, 0.9], 'r:') plt.plot([thresholds_90_precision, thresholds_90_precision],[0.9,0.], 'r:') plt.plot([-50000, thresholds_90_precision],[recall_90_precision,recall_90_precision], 'r:') plt.plot([thresholds_90_precision],[0.9],'ro') plt.plot([thresholds_90_precision],[recall_90_precision],'ro') plt.show() In\u00a0[26]: Copied! <pre>y_train_pred_90 = (y_scores &gt;= thresholds_90_precision)\nprint(precision_score(y_train_5, y_train_pred_90))\nprint(recall_score(y_train_5, y_train_pred_90))\n</pre> y_train_pred_90 = (y_scores &gt;= thresholds_90_precision) print(precision_score(y_train_5, y_train_pred_90)) print(recall_score(y_train_5, y_train_pred_90)) <pre>0.9000345901072293\n0.4799852425751706\n</pre> <p>Receiver Operating Characteristic</p> <p>Ratio of Positive instances correctly classified (as positive) $$ \\text{Recall/Sensitivity} = TPR = \\dfrac{TP}{TP + FN} = \\dfrac{TP}{\\text{Total Actual Positives}} $$ Ratio of Negative instances, incorrectly classified (as positive) $$ FPR = \\dfrac{FP}{TN + FP} = \\dfrac{FP}{\\text{Total Actual Negatives}} $$ Ratio of Negative instances, correctly classified (as negative) $$ \\text{Specificity} = TNR = \\dfrac{TN}{TN + FP} = \\dfrac{TN}{\\text{Total Actual Negatives}} $$</p> <p>How to decide between ROC and PR?</p> <p>If positive class is rare or when we care more about false positives than false negatives (e.g. Is a claim fradulent? Positive = fraudulant. In this case we will choose PR because both FP and FN are costly, but the primary concern is the class imbalance. Positive class is rare and thus we choose PR curves.)</p> <p>Receiver Operating Characteristic - Area under the Curve (ROC AUC) is a metric used to compare different classifiers.</p> <ul> <li>Purely random: AUC = 0.5</li> <li>Perfect Classifier: AUC = 1.0</li> </ul> <p><code>sklearn</code> classifiers generally have one or the other</p> <ul> <li><code>decision_function()</code></li> <li><code>predict_proba()</code></li> </ul> In\u00a0[27]: Copied! <pre># ROC Curve plots sensitivity (recall) vs 1- specificity\nfrom sklearn.metrics import roc_curve\n\nfpr, tpr, thresholds = roc_curve(y_train_5, y_scores)\n</pre> # ROC Curve plots sensitivity (recall) vs 1- specificity from sklearn.metrics import roc_curve  fpr, tpr, thresholds = roc_curve(y_train_5, y_scores) In\u00a0[28]: Copied! <pre>def plot_roc_curve(fpr, tpr, label = None):\n    plt.plot(fpr, tpr, linewidth =2, label=label)\n    plt.plot([0,1],[0,1], 'k--')\n    plt.xlabel(\"False Positive Rate\", fontsize=16)\n    plt.ylabel(\"True Positive Rate (Recall)\", fontsize=16)\n    plt.axis([0,1,0,1])\n    plt.grid(True)\n\n# we select the fpr with 90% Precision\nplt.figure(figsize=(8,6))\nfpr_90_precision = fpr[np.argmax(tpr &gt;= recall_90_precision)]\nplot_roc_curve(fpr, tpr)\nplt.plot([fpr_90_precision,fpr_90_precision],[0, recall_90_precision],'r:')\nplt.plot([fpr_90_precision],[recall_90_precision],'ro')\nplt.show()\n</pre> def plot_roc_curve(fpr, tpr, label = None):     plt.plot(fpr, tpr, linewidth =2, label=label)     plt.plot([0,1],[0,1], 'k--')     plt.xlabel(\"False Positive Rate\", fontsize=16)     plt.ylabel(\"True Positive Rate (Recall)\", fontsize=16)     plt.axis([0,1,0,1])     plt.grid(True)  # we select the fpr with 90% Precision plt.figure(figsize=(8,6)) fpr_90_precision = fpr[np.argmax(tpr &gt;= recall_90_precision)] plot_roc_curve(fpr, tpr) plt.plot([fpr_90_precision,fpr_90_precision],[0, recall_90_precision],'r:') plt.plot([fpr_90_precision],[recall_90_precision],'ro') plt.show() <p>Both the ROC and the Precision-Recall curve give multiple values of fpr, tpr, precision, recall values at different threshold levels. So that we can plot them</p> <p>The tradeoff in ROC is that the higher the recall (TPR), the higher the FPR.</p> <p>A purely random classifier is represented by that dotted line</p> In\u00a0[29]: Copied! <pre>from sklearn.metrics import roc_auc_score\nroc_auc_score(y_train_5, y_scores)\n</pre> from sklearn.metrics import roc_auc_score roc_auc_score(y_train_5, y_scores) Out[29]: <pre>0.9604938554008616</pre> <p>How to decide between ROC and PR?</p> <p>If positive class is rare or when we care more about false positives than false negatives (e.g. Is a claim fradulent? Positive = fraudulant. In this case we will choose PR because both FP and FN are costly, but the primary concern is the class imbalance. Positive class is rare and thus we choose PR curves.)</p> <p>Receiver Operating Characteristic - Area under the Curve (ROC AUC) is a metric used to compare different classifiers.</p> <p>Purely random: AUC = 0.5</p> <p>Perfect Classifier: AUC = 1.0</p> <p><code>sklearn</code> classifiers generally have one or the other</p> <ul> <li><code>decision_function()</code></li> <li><code>predict_proba()</code></li> </ul> In\u00a0[30]: Copied! <pre>from sklearn.ensemble import RandomForestClassifier\n\nforest_clf = RandomForestClassifier(random_state=42)\ny_probas_forest = cross_val_predict(forest_clf, X_train, y_train_5, cv=3, method='predict_proba')\n</pre> from sklearn.ensemble import RandomForestClassifier  forest_clf = RandomForestClassifier(random_state=42) y_probas_forest = cross_val_predict(forest_clf, X_train, y_train_5, cv=3, method='predict_proba') In\u00a0[31]: Copied! <pre>y_scores_forest = y_probas_forest[:, 1]\ny_scores_forest\n</pre> y_scores_forest = y_probas_forest[:, 1] y_scores_forest Out[31]: <pre>array([0.89, 0.01, 0.04, ..., 0.98, 0.08, 0.06])</pre> In\u00a0[32]: Copied! <pre>fpr_forest, tpr_forest, thresholds_forest = roc_curve(y_train_5, y_scores_forest)\n</pre> fpr_forest, tpr_forest, thresholds_forest = roc_curve(y_train_5, y_scores_forest) In\u00a0[35]: Copied! <pre>recall_90_precision_forest = tpr_forest[np.argmax(fpr_forest &gt;= fpr_forest)]\nplt.plot(fpr, tpr, 'b:', label=\"SGD\")\nplot_roc_curve(fpr_forest, tpr_forest, label=\"Random Forest\")\nplt.legend(loc=\"lower right\")\nplt.show()\n</pre> recall_90_precision_forest = tpr_forest[np.argmax(fpr_forest &gt;= fpr_forest)] plt.plot(fpr, tpr, 'b:', label=\"SGD\") plot_roc_curve(fpr_forest, tpr_forest, label=\"Random Forest\") plt.legend(loc=\"lower right\") plt.show() In\u00a0[36]: Copied! <pre>roc_auc_score(y_train_5, y_scores_forest)\n</pre> roc_auc_score(y_train_5, y_scores_forest) Out[36]: <pre>0.9983436731328145</pre> In\u00a0[37]: Copied! <pre>y_pred_forest = cross_val_predict(forest_clf, X_train, y_train_5, cv=3)\nprint(precision_score(y_train_5,y_pred_forest))\nprint(recall_score(y_train_5, y_pred_forest))\n</pre> y_pred_forest = cross_val_predict(forest_clf, X_train, y_train_5, cv=3) print(precision_score(y_train_5,y_pred_forest)) print(recall_score(y_train_5, y_pred_forest)) <pre>0.9905083315756169\n0.8662608374838591\n</pre> <p>Just lost 4 hours of work because I refreshed the Colab notebook :/</p> In\u00a0[38]: Copied! <pre>from sklearn.preprocessing import StandardScaler\n\nscalar = StandardScaler()\nX_train_scaled = scalar.fit_transform(X_train.astype(np.float64))\n</pre> from sklearn.preprocessing import StandardScaler  scalar = StandardScaler() X_train_scaled = scalar.fit_transform(X_train.astype(np.float64)) In\u00a0[39]: Copied! <pre>from sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.linear_model import SGDClassifier\nsgd_clf = SGDClassifier(random_state=42)\ny_train_pred = cross_val_predict(sgd_clf, X_train_scaled, y_train, cv=3)\n</pre> from sklearn.model_selection import cross_val_predict from sklearn.metrics import confusion_matrix from sklearn.linear_model import SGDClassifier sgd_clf = SGDClassifier(random_state=42) y_train_pred = cross_val_predict(sgd_clf, X_train_scaled, y_train, cv=3) In\u00a0[40]: Copied! <pre>conf_mx = confusion_matrix(y_train, y_train_pred)\nconf_mx\n</pre> conf_mx = confusion_matrix(y_train, y_train_pred) conf_mx Out[40]: <pre>array([[5577,    0,   22,    5,    8,   43,   36,    6,  225,    1],\n       [   0, 6400,   37,   24,    4,   44,    4,    7,  212,   10],\n       [  27,   27, 5220,   92,   73,   27,   67,   36,  378,   11],\n       [  22,   17,  117, 5227,    2,  203,   27,   40,  403,   73],\n       [  12,   14,   41,    9, 5182,   12,   34,   27,  347,  164],\n       [  27,   15,   30,  168,   53, 4444,   75,   14,  535,   60],\n       [  30,   15,   42,    3,   44,   97, 5552,    3,  131,    1],\n       [  21,   10,   51,   30,   49,   12,    3, 5684,  195,  210],\n       [  17,   63,   48,   86,    3,  126,   25,   10, 5429,   44],\n       [  25,   18,   30,   64,  118,   36,    1,  179,  371, 5107]],\n      dtype=int64)</pre> In\u00a0[41]: Copied! <pre>plt.matshow(conf_mx, cmap=plt.cm.gray)\nplt.show()\n</pre> plt.matshow(conf_mx, cmap=plt.cm.gray) plt.show() In\u00a0[42]: Copied! <pre>row_sums = conf_mx.sum(axis=1, keepdims=True)\nnorm_conf_mx = conf_mx / row_sums\n</pre> row_sums = conf_mx.sum(axis=1, keepdims=True) norm_conf_mx = conf_mx / row_sums In\u00a0[43]: Copied! <pre>np.fill_diagonal(norm_conf_mx, 0)\nplt.matshow(norm_conf_mx, cmap=plt.cm.gray)\nplt.show()\n</pre> np.fill_diagonal(norm_conf_mx, 0) plt.matshow(norm_conf_mx, cmap=plt.cm.gray) plt.show() In\u00a0[44]: Copied! <pre>from sklearn.neighbors import KNeighborsClassifier\n\ny_train_large = (y_train &gt;= 7)\ny_train_odd = (y_train % 2 == 1)\ny_multilabel = np.c_[y_train_large, y_train_odd]\n\nknn_clf = KNeighborsClassifier()\nknn_clf.fit(X_train, y_multilabel)\n</pre> from sklearn.neighbors import KNeighborsClassifier  y_train_large = (y_train &gt;= 7) y_train_odd = (y_train % 2 == 1) y_multilabel = np.c_[y_train_large, y_train_odd]  knn_clf = KNeighborsClassifier() knn_clf.fit(X_train, y_multilabel) Out[44]: <pre>KNeighborsClassifier()</pre> In\u00a0[\u00a0]: Copied! <pre>knn_clf.predict([some_digit])\n</pre> knn_clf.predict([some_digit]) In\u00a0[46]: Copied! <pre>y_train_knn_pred = cross_val_predict(knn_clf, X_train, y_multilabel, cv=3)\n</pre> y_train_knn_pred = cross_val_predict(knn_clf, X_train, y_multilabel, cv=3) In\u00a0[47]: Copied! <pre>from sklearn.metrics import confusion_matrix\ny_train_perfect_predictions = y_train_5\nconfusion_matrix(y_train_5, y_train_perfect_predictions)\n</pre>  from sklearn.metrics import confusion_matrix y_train_perfect_predictions = y_train_5 confusion_matrix(y_train_5, y_train_perfect_predictions) Out[47]: <pre>array([[54579,     0],\n       [    0,  5421]], dtype=int64)</pre> <p>Each row represents actual class (Positive / Negative). Each column represents predicted class (+ / -)</p> In\u00a0[48]: Copied! <pre>from sklearn.metrics import f1_score\n\nf1_score(y_multilabel, y_train_knn_pred, average='macro')\n</pre> from sklearn.metrics import f1_score  f1_score(y_multilabel, y_train_knn_pred, average='macro') Out[48]: <pre>0.976410265560605</pre> <p>Each label can be multiclass, each can have more than two possible values. In other words</p> <p>$$ \\text{Multilabel} + \\text{Multiclass} = \\text{Multioutput}  $$</p>","tags":["Machine Learning"]},{"location":"ML/handson/03/#classification","title":"Classification\u00b6","text":"","tags":["Machine Learning"]},{"location":"ML/handson/03/#binary-classifier","title":"Binary Classifier\u00b6","text":"","tags":["Machine Learning"]},{"location":"ML/handson/03/#stochastic-gradient-descent-sgd-classifier","title":"Stochastic Gradient Descent (SGD) Classifier\u00b6","text":"<ul> <li>can handle extremely large datasets</li> </ul>","tags":["Machine Learning"]},{"location":"ML/handson/03/#performance-measures","title":"Performance Measures\u00b6","text":"","tags":["Machine Learning"]},{"location":"ML/handson/03/#cross-validation","title":"Cross Validation\u00b6","text":"<p>Same as done before, in chapter 2</p>","tags":["Machine Learning"]},{"location":"ML/handson/03/#precision-recall","title":"Precision &amp; Recall\u00b6","text":"<p>Precision refers to \"How many times were you right about a Positive?\" This measures the accuracy of positive predictions.</p> <p>$$ \\text{precision} = \\frac{TP}{TP + FP} $$</p> <p>Recall refers to \"How many positives could you recall correctly?\". Also refers to sensitivity or True Positive Rate (TPR).  Another way to think about recall is how well you are able to DETECT the positives? Are you able to detect all of them or are there a few misses?</p> <p>$$ \\text{recall} = \\frac{TP}{TP + FN} $$</p> <p>Why do we need both precision and recall?</p> <p>Think of it this way. If I correctly identified 10 positives, $TP = 10$. Say, I was very careful, so I didn't make any false positives, $FP = 0$ (e.g. never deem the innocent guilty even if it means you will have to let go of some criminals), so $FN \\neq 0$.</p> <p>This means our precision remains $100\\%$ (careful to ensure there are no $FP$) but our recall would reduce, since we are letting some criminals go ($FN \\neq 0$).</p> <ul> <li>Need high precision when its important not to have any False positives (mark as safe. Like 'safe for children' videos or 'safe for eating' or 'safe from virus' to allow someone into your house)</li> <li>Need high recall when its important to catch shoplifters, it will be more inconvenient but its more important to \"GET ALL THE POSITIVES RIGHT\", even when there are more false positives.</li> </ul>","tags":["Machine Learning"]},{"location":"ML/handson/03/#precision-recall-tradeoff","title":"Precision-Recall Tradeoff\u00b6","text":"","tags":["Machine Learning"]},{"location":"ML/handson/03/#the-roc-curve","title":"The ROC Curve\u00b6","text":"","tags":["Machine Learning"]},{"location":"ML/handson/03/#multivariate-classification","title":"Multivariate Classification\u00b6","text":"<p>There are multiple types of classifiers:</p> <ul> <li>SGD</li> <li>Random Forest Classifier</li> <li>Naive Bayes</li> </ul> <p>These have native support for multiple classes. But there are others that don't like</p> <ul> <li>Support Vector Machine Classifiers (SVCs)</li> <li>Logistic Regressions</li> </ul> <p>They are strictly binary in nature. However, we can use alternate strategies like <code>OvR</code> (One vs Rest) and <code>OvO</code> (One vs One) strategy in order to use these models for multivariate classification. Say, we have $N$ classes (each having $\\frac{1}{N}$ portion in the data)</p> <ul> <li><code>OvR</code> fits $N$ binary models. Each model checking for one (positive) and the rest (negative). So, on each model we have to fit 100% of the data.</li> <li><code>OvO</code> fits $\\dbinom{N}{2}$ binary models, $(0,1), (0,2)\\dots(1,2),(1,3)\\dots$ i.e. pairs of each class against others. Thus each model we have to fit contains only 20% of the data. But the number of models to be fit would be huge (e.g. 10 classes will require 45 models to be fit). So, this can be utilized by models that don't scale well with data. E.g. <code>SVC</code>s.</li> </ul>","tags":["Machine Learning"]},{"location":"ML/handson/03/#variations","title":"Variations\u00b6","text":"<ul> <li>Multiclass can have multiple classes (e.g. 0,1,2,3...) for the singular label that is being predicted.</li> <li>Multilabel  can have multiple labels (each having a binary class)</li> <li>Multioutput is a generalization of multilabel where there are multiple labels and each label can have multiple classes.</li> </ul> <p>In essence, every model is just a specification of a multioutput model. $$ \\text{Multilabel} + \\text{Multiclass} = \\text{Multioutput}  $$</p> <p>The author shows an insane example for multioutput where he trains a model to remove the noise from a noisy digit using the <code>knn</code> classifier.</p>","tags":["Machine Learning"]},{"location":"ML/handson/03/#multilabel-classification","title":"Multilabel classification\u00b6","text":"","tags":["Machine Learning"]},{"location":"ML/handson/03/#f_1-score","title":"$F_{1}$ score\u00b6","text":"<p>This is the harmonic mean of the precision and recall and gives more value to lower values. So, we get a higher $F_{1}$ score only when both precision and recall are higher $$ F_{1} = \\dfrac{2}{\\dfrac{1}{\\text{Precision}} + \\dfrac{1}{\\text{Recall}}} $$</p>","tags":["Machine Learning"]},{"location":"ML/handson/03/#confusion-matrix","title":"Confusion Matrix\u00b6","text":"<p>The confusion matrix gives a good idea about the accuracy of binary classifiers</p>","tags":["Machine Learning"]},{"location":"ML/handson/03/#multioutput-classification","title":"Multioutput Classification\u00b6","text":"","tags":["Machine Learning"]},{"location":"blog/","title":"Writings on the Wall","text":"<p>Written. REwritten. Not REvised (Lazy)</p>"},{"location":"blog/2025/blog-workflow/","title":"My new blogging workflow works extremely well!","text":"<p>I swear, I didn't imagine to come this far with my current blogging setup with which I am extremely pleased! I will share with you what I have been able to do with my current setup</p> <ol> <li>Set up an obsidian vault with plugins like<ol> <li>\\(\\LaTeX\\) suite<sup>3</sup></li> <li>Longform (for taking care of boilerplate stuff like setting up templates)</li> <li>Prism theme (for the focused minded)<sup>1</sup></li> <li>Git (plugin)</li> <li>Footnote Shortcut<sup>2</sup></li> </ol> </li> <li>Use mkdocs/material as the SSG of choice, and configure it to my heart's content. Use plugins as per liking, and create custom layouts for the homepage (which IMHO looks pretty cool!)</li> <li>Create two repositories<ol> <li>Private repo: For all the content, work in progress, config files, custom layouts etc so that in case I can maintain an all-in-one private vault with a single folder <code>docs/</code> which contains all my publishables.</li> <li>Public repo: For pushing the built site and using <code>hgup.github.io</code> as its name to allow github pages to work seamlessly. It also houses all the <code>giscus</code> comments.</li> </ol> </li> <li>Create a kickass CICD GitHub Actions file</li> </ol> <p>The GitHub Actions is my favorite part. See I have two requirements (not really requirements, but nice-to-haves).</p> <ol> <li>I have private files in my vault that I don't want to be published or viewed by anyone and also when I push such files to my repository. I don't need the entire site to be built again with no real changes (since I hadn't updated any files within the <code>docs/</code> folder nor the <code>mkdocs.yml</code> file) and really its bad for the environment (using computing power for nothing)</li> <li>Whenever I update a file in the<code>docs/</code> folder I want the CICD pipeline to run so that the changes can be LIVE on hgup.github.io</li> </ol> <p>This <code>ci.yml</code> actions file does it all!</p> .github/workflows/ci.yml<pre><code>name: ci \non:\n  push:\n    branches:\n      - master \n      - main\npermissions:\n  contents: write\njobs:\n  publish:\n    name: Publish\n    # The extremely crucial conditional statement (1)\n    if: \"contains(github.event.head_commit.message, 'docs/') || contains(github.event.head_commit.message, 'mkdocs.yml') || contains(github.event.head_commit.message, 'config')\"\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - name: Configure Git Credentials\n        run: |\n          git config user.name github-actions[bot]\n          git config user.email 41898xxx+github-actions[bot]@users.noreply.github.com\n      - uses: actions/setup-python@v5\n        with:\n          python-version: 3.x\n      - run: echo \"cache_id=$(date --utc '+%V')\" &gt;&gt; $GITHUB_ENV \n      - uses: actions/cache@v4\n        with:\n          key: mkdocs-material-${{ env.cache_id }}\n          path: .cache \n          restore-keys: |\n            mkdocs-material-\n      - run: pip install mkdocs-material \n      - run: pip install mkdocs-awesome-nav \n      - run: pip install \"mkdocs-material[imaging]\"\n\n      - name: Build MkDocs site\n        run: mkdocs build # This creates the 'site' directory\n\n      - name: Push built files to target repository # (2)\n        uses: cpina/github-action-push-to-another-repository@v1.7.2\n        env:\n          API_TOKEN_GITHUB: ${{ secrets.DEPLOY_PAT }} # Use your secret PAT (3)\n        with:\n          # Source directory from the origin directory\n          source-directory: 'site'\n          # Name of the destination username/organization\n          destination-github-username: 'hgup'\n          # Destination repository\n          destination-repository-name: 'hgup.github.io'\n          # Email for the git commit\n          user-email: 'action@github.com'\n          # [Optional] set target branch name for the destination repository. Defaults to \"main\"\n          target-branch: 'gh-pages' # optional, default is main\n          # [Optional] create target branch if not exist. Defaults to `false`\n          create-target-branch-if-needed: true # optional\n          commit-message: 'Site update'\n</code></pre> <ol> <li>This condition runs the publishing sequence only if the commit message contains the following three strings. <code>docs/</code>,<code>mkdocs.yml</code> and <code>config</code>. I have set up Git on obsidian in such a way that it adds the file names to the description of the commit and thus whenever I update a file in the <code>docs/</code> folder, it registers and triggers a build. Pretty neat and satisfies the two nice-to-haves above.</li> <li>The built files are then pushed to my public repository <code>hgup.github.io</code>. This ensures that I don't have to make my entire vault public and only the publishable stuff exists in the public repository.</li> <li>I am using fine-grained tokens here and only allowed this bot to update the <code>content</code> of this particular repository <code>hgup.github.io</code>. So, I don't need to worry about it being leaked really.</li> </ol> <p>And that's it! Now that I have written this post. I will just <code>commit-and-sync</code> using obsidian and you should be able to see it live on hgup.github.io.</p> <ol> <li> <p>I wanted to enjoy the process of writing and forget that I ever published anything online. I wanted to just open the site one day feel surprised \"Wow! All this exists??\"\u00a0\u21a9</p> </li> <li> <p>(with a keyboard shortcut <code>Alt+Q</code>) which I used to create this footnote too\u00a0\u21a9</p> </li> <li> <p>This also contains many snippets that allows me to utilize mkdocs's extra features without remember syntax too much.\u00a0\u21a9</p> </li> </ol>","tags":["Setup","Workflow"]},{"location":"blog/2025/dont-wait/","title":"Don't wait for it. Just do it","text":"<p>We wait for the right moment for the right things to happen. But what exactly is the \"right time\"?</p> <p>Isn't it true that a swordsman who waited for years for the battle let his sword rust by saying \"I will only ever wield it when the time is right\" and breaks it in the battle.</p> <p>Isn't it true that there can always be a little better situation than the current one. In that case isn't it wise to have started the deed a bit later than now. But what if things go the other way round? Then the right moment would have been now, but we will never know what tomorrow has in stock for us.</p> <p>So, the best thing we can do is to have a basic plan<sup>1</sup> and move ahead</p> <pre><code>flowchart LR\n    A([\"Start\"])\n    A --&gt; B{\"Wait\"}\n    B --\"Yes\"--&gt; C[/Keep Waiting/]\n    B --\"No\"o--&gt; D[/Seize the moment/]</code></pre> <p>Hope it makes sense one day. Peace.</p> <p>Note</p> <p>This is just a random post. Consider it a parody or satire. Idc.</p> <ol> <li> <p>We can go on thinking about the \"perfect\" plan and then doom us in a similar way.\u00a0\u21a9</p> </li> </ol>"},{"location":"blog/2025/finally-mkdocs/","title":"Why I shifted to mkdocs and it just works!","text":"<p>I have been shifting quite a lot between different options for sharing written content online. It should cater to all of these needs</p> <ul> <li>Regular blogs</li> <li>Walkthroughs (e.g. creating a guide for Actuarial Exams)</li> <li>Highly customizable</li> <li>(yet) Easy to maintain</li> </ul> <p>I started by copying Lee Rob's portfolio website (that also helped me learn Next.js). But to add a new feature, add a new blog post was a big headache.</p> <p>Then, I started using Quartz (since I was using a lot of Obsidian, and it was a no-brainer. That's when I also realized that the system that should be used, should be extremely frictionless and content-focused). However it was not customizable and I found it a bit weird and templatey.</p> <p>Then I tried making two websites on Svelte, taking inspiration of a design from Dribble. But I eventually realized that I was going back to step 1. It was just too hard to maintain.</p> <p>So, I needed a readymade solution. Since, I was already motivated to learn Svelte Kit, I tried looking up someone else's blog example implemented in sveltekit and stumbled upon urara.dev. But it was just someone's implementation and was buggy.</p> <p>I realized that I didn't need something that was too flashy. But something that just works and works very well for all my requirements.</p> <p>That's when I found out about Material theme for mkdocs (which is the website you are reading this blog on). And it was a one-time effort to set it up. After that, all I needed to do was focus on my content. That's all!</p>"},{"location":"blog/2025/github-pages-search-engine-name/","title":"Show intended GitHub pages name in Google Search","text":"<p>While setting up the site and publishing it on GitHub Pages. I observed that the search engine results were not showing up as per my liking.</p> <p>Let me show you an example of what I mean:</p> <p></p> <p>Google shows \"MS 365 &amp; Power Platform Community\" correctly but \"GitHub\" and \"github.io\" for the others </p> index.html<pre><code>&lt;script type='application/ld+json'&gt;\n{\n    \"@context\" : \"http://schema.org\",\n    \"@type\" : \"WebSite\",\n    \"name\" : \"Microsoft 365 &amp;amp; Power Platform Community\",\n    \"url\" : \"https://pnp.github.io/\",\n    \"description\" : \"Learn from others how to build apps on Microsoft 365 &amp;amp; Power Platform.\",\n    \"thumbnailUrl\" : \"https://pnp.github.io/pnp.svg\",\n    \"alternateName\" : \"PnP\"\n\n}\n&lt;/script&gt;\n</code></pre> <p>pnp.github.io/index.html on GitHub</p> <p>This made me curious and after a quick Gemini search, I found out that it's a structured data markup in JSON-LD format that provides information about a website to search engines. Meaning:</p> <code>\"@context\" : \"http://schema.org\"</code> This specifies that the vocabulary used in this data is from Schema.org, a collaborative community activity whose mission is to create, maintain, and promote schemas for structured data on the Internet, on web pages, in email messages, and beyond. <code>\"@type\" : \"WebSite\"</code> This indicates that the entity being described is a website. <code>\"name\" : \"Hursh Gupta\"</code> The name of the website. <p>In essence, this script helps search engines like Google understand the content and purpose of the website more effectively, which can improve its visibility and presentation in search results.</p> <p>So, without wasting any time, I added this to my site too. What I expect to see is the \"GitHub\" turning to \"Hursh Gupta\". Let's wait...</p> <p></p> <p>Before changing it (23 June, 2025)</p> <p>Waiting for site to update. will update you IF it happens</p>"},{"location":"blog/2023/how-do-i-balance-it/","title":"How do I balance it?","text":"<p>Let me tell you a short story. Really, a short one. I wake up at 3 a.m., brush and sit to study in a room quite isolated from the inmates of the hostel. Why? Because, I have a very fickle mind, focus is hard to get. But once I get it, I can do a good magnitude of work without being disturbed.</p> <p></p> <p>But why do I need to get up at 3? Just for the feel of it? No.</p> <p>I am part of our Institute brass band. This means commitment. We practice in the mornings and evenings while the rest of the hostel have their games and study hours respectively. Band is fun, and that reason alone is enough for us to spend our time in this rather than that. However, when you have Actuarial exams to clear, things get a little tricky.</p> <p>You have to find time for your university academics and also practice questions for your upcoming MAS-I and/or MAS-II exams. It means business.</p> <p>So there are three things on my plate</p> <ul> <li>The Brass Band</li> <li>University Exams</li> <li>Actuarial Exams</li> </ul> <p>But this isn\u2019t the end. We follow a tight schedule day in and out, in our hostel at SSSIHL. The main motivation of studying in that place is getting closer to God, as without his grace none of what I am about to tell you would have been possible.</p> <p>So, after college, we are given sufficient time to relax, have tea and then we head for Mandir to sing Bhajans (devotional songs) which if done correctly, gives us the energy to fight against whatever problems we are facing in our lives.</p> <p>Thus if I can consolidate all the \u201cfree\u201d timings that I have in a day, we get:</p> <pre><code>8:00 \u2013 8:45 am (post breakfast)\n12:30 \u2013 1:00 pm (post lunch)\n4:00 \u2013 5:20 pm (tea time)\n6:45 \u2013 7:15 pm (after Mandir)\n7:30 \u2013 8:00 pm (after Dinner)\n</code></pre> <p></p> <p>Quite fragmented right? So, you always have to be in the zone; keep planning about the next step and allow not even a single minute to be wasted. Now that I reflect back, I realise how our institute trains our mind to achieve things even when time is less. This turned out to be a blessing as we will see later.</p> <p>We normally have study hours from 8:00 to 9:40 pm, during which our classmates will be reading academic textbooks and we will be reading music scores.</p> <p>So, what do I do now? I might sound like I am complaining, and it might be true that I was, back then. But what I want to highlight is that when we have a lot on our plate, we ourselves are to blame, and we will have to prioritize and get things done. Otherwise we shouldn\u2019t have opted for it in the first place.</p> <p></p> <p>I chose this institute[^1] because I loved the system (and eventually the reason turned to God). I opted in for the Band because I loved music. I chose Actuarial Science because I was highly inspired by my mentor and my seniors (both students and working Actuaries).</p> <p>I cannot go back. All I needed to do is balance.</p> <p>But wait, it isn\u2019t over yet, I have to revise the list:</p> <ul> <li>The Brass Band</li> <li>University Exam: Takes place in November 2022, just after my Actuarial Exam</li> <li>Actuarial Exam: Writing MAS-I in the November 2022 diet</li> <li>Hostel events</li> </ul> <p>We love to say that our hostel is a place where\u2026</p> <p>\u201cEach lives for the other and all live for God.\u201d</p> <p>We have many subsequently occuring events. They may be house events (like Gryffindor) consisting of Drama, Orchestra, Sports etc. Or they may be festivals like Krishna Asthami, Dusshera and many more. During those events we all come together, participate and give it our best.</p> <p>All our house events take place in the odd semster, and this year especially it was very tightly packed. For instance, this year the orchestra and drama events were spaced a week apart. And we had been told about the dates at a very short notice.</p> <p>Now, I had to choose. I couldn\u2019t do everything, I had to say \u201cno\u201d to something. Obviously I couldn\u2019t cross out the first two items in the list above. So the choice was between writing MAS-I in November or participating in house events.</p> <p>I thought for a while, prayed even and then came to the resolution that its best to skip the Actuarial Exam diet and work hard in the Hostel events. Why? Because, if not then what\u2019s the point? (Yes, referencing you, Stories by Shriram )</p> <p>My reasoning was not flawed. The original plan was to write MAS-I in the odd semester and MAS-II in the even semester. But now that the odd semester was crowded with events, I wouldn\u2019t be able to do well in everything.  </p> <p>But, the even semester was very free (and we wouldn\u2019t have band practice for events during this time). So, there was not much to think about, I had to write two exams in the May 2023 diet.</p> <p>At the end we lost the house cup (and also both the Drama and Orchestra events in which I had actively participated). But we did make memories.</p> <p>After the odd-sem ended, we conduct our sports meet, the last event in which the Band performs. After that got over, I had time to study.</p> <p>But first, I had to prepare for MAS-I. As time flew past quickly, I developed the conciousness that I was not well prepared in MAS-I and the exam was nearing quick. I had other peers who had written and cleared this exam and now they were preparing for MAS-II.</p> <p>Then I consoled myself, that I won\u2019t be able to write both exams in this diet, as I felt sad about it.</p> <p>Enters February. Since, I had only one exam to prepare for I was quite relaxed and prepared nicely for MAS-I. But then I got an itch, \u201cI have so much time now, can\u2019t I study for the other exam too?\u201d I think it was a byproduct of the training I got in the last semester, the blessing I was talking about before.</p> <p>One day, I saw one of my peers studying for MAS-II and asked him how tough is it. Then I took the material and started reading. \u201cMmm\u2026 I think I can do it\u201d, I thought to myself. So, I started preparing silently, without any hopes of writing the exam. Why? Divine intervention. I believe that Swami (how I refer to God) put this thought in my head.</p> <p>This continued. But there was another thing added to our plate. We had sent a video introducing the Actuarial members in our Brass Band to the admins of the Global Conference of Actuaries 2023. They invited us to perform in the Actuarial Gala Function &amp; Awards (AGFA) that would take place in the GCA in March 2023.</p> <p>Super excited as we were, I knew this would mean a lot of study time being spent in practice. Yet, we whole heartedly practiced and it was worth it.</p> <p></p> <p>After returning from Delhi, I knew that it was time for me to go full throttle with my studies. And now that I was a bit involved with MAS-II, I thought, I might actually be able to do it. So, I started waking up earlier and conserved my energy throughout the day to focus on my studies and get through without dozing off.</p> <p>This is why I started waking up at 3.</p> <p>I simultaneously prepared for both the exams. Practicing sample exams. Realising that I am underprepared for MAS-I, I started working hard at that too. Times were not pretty, I tell you. I was even more worried as I had only one month left. But since, I had paid the fees for both the exams, I continued preparing.</p> <p>In the end, I passed both the exams.</p> <p></p>","tags":["Repost"]},{"location":"blog/2025/lessons-from-statistics/","title":"Lessons from Statistics","text":"<p>This is prolly going to be the longest post (over time). I will add whatever clicks to me as I learn stuff while studying actuarial science.</p> <ul> <li>MSE is just the variance in the modelling perspective #fact-check . When you read \"standard error\", it is just standard deviation.</li> <li>We often come across statements like \"Using more parameters gives an advantage in fitting but a disadvantage in prediction\". The reason for that is for every parameter that we wish to estimate we bring in a certain amount for variance (inherent to the estimation process). So, the more the number of parameters that we wish to estimate, the more flexible our estimates will be but the flexibility leads to more variability. <ul> <li>The variance is not just added up, but have a nested dependency. For example, if the \\(X\\) follows a  Poisson distribution and the \\(\\mu = E[X]\\) follows another distribution (say Weibull) which has its own parameters to be estimated from the data, \\(\\theta\\) and \\(\\omega\\). The variance of the these parameters will exaggerate the variance of the parameter \\(\\mu\\). Nested Variance!</li> </ul> </li> <li> <p>What is \\(\\alpha\\) in hypothesis testing? It is the false-positive rate. Say, we were trying to find out if correlation exists? So we set the null hypothesis like so:</p> \\[ \\begin{align} H_{0}&amp;: \\text{Correlation doesn't exist} \\\\ H_{1}&amp;: \\text{It exists!} \\end{align} \\] <ul> <li>We then take a sample from the population which is assumed to have \\(H_{0}\\) and we calculate a test statistic based on that assumption.</li> <li>There are two ways in which this can go. Say we set our \\(\\alpha = 10\\%\\)<ul> <li>Population actually has \\(H_{1}\\) (in which case we hope that the sample gives evidence against \\(H_{0}\\))</li> <li>Population actually has \\(H_{0}\\). But there is a \\(10\\%\\) chance that the sample drawn from the population will betray the population and show evidence against \\(H_{0}\\). And if we believe it, its a false positive.</li> </ul> </li> <li>To sum it up. \\(\\alpha\\) is just the probability that we will draw a sample from the population that will make use falsely reject \\(H_{0}\\). And we have to be ready to take that degree of risk. That is why it is suggested to take multiple samples if possible to perform these tests (think about it, if you perform the hypothesis on \\(k\\) samples, your \\(\\alpha_{k} = \\alpha^k\\))</li> </ul> </li> </ul>"},{"location":"blog/2025/math-in-mdsvex/","title":"Math in `mdsvex`","text":"<p>In order to make it happen, all you need to do is to follow this particular tutorial:</p> <p>https://mdsvex-math-starter.vercel.app/ It's really that simple. The mistake I did was to NOT follow the version of remark math mentioned here.</p> <pre><code># run the Svelte CLI and follow the prompts\nnpx sv create\n# enter and install the plugins\ncd myProject\nnpm i -D remark-math@3 # (1)\nnpm i -D rehype-mathjax\n</code></pre> <ol> <li>Here they are specifically using <code>v3</code>. Pay attention!</li> </ol> <p>With version 3 of remark math, it works as expected! I prefer using <code>rehype-mathjax</code> as it gives matrices and stuff much better than what <code>katex</code> provides, in my experience.</p>","tags":["Math"]},{"location":"blog/2025/mental-keyboard/","title":"The Mental Keyboard","text":"<p>You should be able to run a keyboard in your mind. I try doing it as a mental exercise and see what works and what doesn't.</p> <ol> <li>First imagine all the white and black keys in a keyboard. You should use the black keys to identify the white keys (like if the the white note after the third consecutive black note is a \"B\") </li> </ol> <ol> <li>Then try to identify all the intervals on a keyboard. For example, highlight the notes <code>CG, DA, EB, FC, GD, AE, BGb</code>. You can use some mental shortcuts like<ul> <li>CG Prakash</li> <li>DA: District Attorney</li> <li>EB: Don't ebb around</li> <li>FC: Fan Club</li> <li>GD: Group Discussion</li> <li>AE: \"Aee, don't behave like that\"</li> <li>BGb: A flat BGM (Background music) These ones are completely random, and you can use any that is convenient for you. Just imagine the notes being highlighted and do that more to get quicker at it.</li> </ul> </li> <li>Now, recognize that these are actually in the circle of fifths and recognize where each of these intervals lie in the circle of fifths. This way you will have an idea about where the notation lies on the keyboard vs the circle.</li> </ol>","tags":["Mind","Music"]},{"location":"blog/2025/music-tools/","title":"Music Tools","text":"","tags":["Music"]},{"location":"blog/2025/music-tools/#spitfire-labs","title":"Spitfire LABS","text":"<p>An audio library that gives you experimental ahh vibes. The library is so vast that I just started off by downloading whatever sounded interesting to me.</p> <p>They just revamped the app too. Now it looks much easier to use than the previous version. I like the green theme they are going for. And the audio samples are as High-res as ever. They can go upto 4-5 gigs. So ensure you have the right kind of storage.</p> <p>I also go to <code>profile &gt; Content &gt; Default Download location</code> and change it to a custom folder that exists on my other drive so that I don't need to download it again and again every time I switch machines. You know.</p>","tags":["Music"]},{"location":"blog/2025/music-tools/#musescore-studio","title":"MuseScore Studio","text":"<p>Since I joined band back in my second year of Undergrad, I had been exposed to this rich world of musical scores. We had people in the band who arranged original music, a signature of ours being renditions of bhajans. Historically, Finale has been used for arranging. I really liked its fast (unintuitive) UI, but the key bindings, once you get used to it makes you very productive.</p> <p>However some people liked MuseScore 3, which was good. But I wasn't ready to switch. MuseScore 4 came out, but it still didn't cut it because I could feel how sluggish it was in terms of performance.</p> <p>But when Musescore 4.5 dropped, it was game changing. I think this is the biggest update MuseScore has ever received and makes it much more usable, especially when palette searching doesn't take 2 seconds on every keystroke.</p>","tags":["Music"]},{"location":"blog/2025/new-device-setup/","title":"How I set up my new Windows machine","text":"<p>This is for a windows installation.</p> <p></p>","tags":["Setup"]},{"location":"blog/2025/new-device-setup/#fresh-installation","title":"Fresh Installation","text":"","tags":["Setup"]},{"location":"blog/2025/new-device-setup/#debloat-windows","title":"Debloat windows","text":"<p>Windows 11 is full of bloatware. NOT ACCEPTABLE! Remove all unnecessary stuff that are thrusted into our laptops/PCs by Microsoft.<sup>1</sup></p> <p>So, we will run this script from Raphire/Win11Debloat.</p> <pre><code># Should work unless outdated, else use the link above\n&amp; ([scriptblock]::Create((irm \"https://debloat.raphi.re/\")))\n</code></pre> <ul> <li>If you see a warning related to <code>winget</code> in the blue window, then install the app installer</li> <li>Go with recommended settings (But do suggest you to clean out the taskbar and start menu.)</li> </ul>","tags":["Setup"]},{"location":"blog/2025/new-device-setup/#realtek-hd-audio-manager","title":"Realtek HD Audio Manager","text":"<p>I remember using the Realtek HD Audio Manager back when Windows 7 was peak. The manager allowed us to set default devices and have some low latency output monitoring</p>","tags":["Setup"]},{"location":"blog/2025/new-device-setup/#other","title":"Other","text":"<pre><code>Set-ExecutionPolicy -Scope CurrentUser -ExecutionPolicy Unrestricted\n</code></pre>","tags":["Setup"]},{"location":"blog/2025/new-device-setup/#apps","title":"Apps","text":"<p>Here are the apps that I use on my system. One suggestion is to use apps that do most of the things that you require to do.</p>","tags":["Setup"]},{"location":"blog/2025/new-device-setup/#daily","title":"Daily","text":"<ul> <li>Arc Browser (Not anymore, meet Zen Browser which is much faster!)</li> <li>Dropbox</li> </ul>","tags":["Setup"]},{"location":"blog/2025/new-device-setup/#academics","title":"Academics","text":"<ul> <li>Zotero</li> <li>Obsidian</li> </ul>","tags":["Setup"]},{"location":"blog/2025/new-device-setup/#dev","title":"Dev","text":"","tags":["Setup"]},{"location":"blog/2025/new-device-setup/#python","title":"Python","text":"<p>For using Tensorflow 2 in machine learning projects, we can download miniconda to get python installed and then load up this environment. This can be activated from anywhere and be used for all datascience projects. A really good base which includes most of the things needed</p> <pre><code>conda env create --file=https://raw.githubusercontent.com/ageron/handson-ml2/refs/heads/master/environment.yml python=3.8\n</code></pre>","tags":["Setup"]},{"location":"blog/2025/new-device-setup/#vs-code","title":"VS Code","text":"<p>I use the vim extension with VS Code. These are my settings and keybindings.</p> settings.json<pre><code>{\n  \"vim.easymotion\": true,\n  \"vim.incsearch\": true,\n  \"vim.useSystemClipboard\": true,\n  \"vim.hlsearch\": true,\n  \"vim.insertModeKeyBindings\": [\n    {\n      \"before\": [\n        \"j\",\n        \"j\"\n      ],\n      \"after\": [\n        \"&lt;Esc&gt;\"\n      ]\n    }\n  ],\n  \"vim.leader\": \"&lt;space&gt;\",\n  \"vim.handleKeys\": {\n    \"&lt;C-a&gt;\": false,\n    \"&lt;C-shift-e&gt;\": false,\n    \"&lt;C-f&gt;\": false,\n    \"&lt;C-p&gt;\": false,\n    \"&lt;C-w&gt;\": false,\n    \"&lt;C-shift-p&gt;\": false,\n  },\n  \"vim.normalModeKeyBindingsNonRecursive\": [\n    {\n      \"before\": [\n        \"Ctrl+e\"\n      ],\n      \"commands\": [\n        \"workbench.files.action.focusFilesExplorer\"\n      ]\n    },\n    // {\n    //   \"before\": [\":\"],\n    //   \"commands\": [\"workbench.action.showCommands\"]\n    // }\n  ],\n  \"vim.visualModeKeyBindingsNonRecursive\": [\n    {\n      \"before\": [ \"&gt;\" ],\n      \"commands\": [ \"editor.action.indentLines\" ]\n    },\n    {\n      \"before\": [ \"&lt;\" ],\n      \"commands\": [ \"editor.action.outdentLines\" ]\n    },\n    // {\n    //   \"before\": [ \"p\" ],\n    //   \"after\": [ \"p\", \"g\", \"v\", \"y\" ]\n    // }\n  ],\n  //   \"vim.statusBarColorControl\": true,\n  //   \"vim.statusBarColors.normal\": [\"#1f1f1f\", \"#ddd\"],\n  //   \"vim.statusBarColors.insert\": [\"9f9f9f\", \"#000\"],\n  //   \"vim.statusBarColors.visual\": [\"#B48EAD\", \"#000\"],\n  //   \"vim.statusBarColors.visualline\": [\"#B48EAD\", \"#000\"],\n  //   \"vim.statusBarColors.visualblock\": [\"#A3BE8C\", \"#000\"],\n  // \"vim.statusBarColors.replace\": \"#D08770\",\n  // \"vim.statusBarColors.commandlineinprogress\": \"#007ACC\",\n  // \"vim.statusBarColors.searchinprogressmode\": \"#007ACC\",\n  // \"vim.statusBarColors.easymotionmode\": \"#007ACC\",\n  // \"vim.statusBarColors.easymotioninputmode\": \"#007ACC\",\n  // \"vim.statusBarColors.surroundinputmode\": \"#007ACC\",\n}\n</code></pre> keybindings.json<pre><code>[\n    {\n        \"key\": \"ctrl+e\",\n        \"command\": \"workbench.action.focusActiveEditorGroup\",\n        \"when\": \"filesExplorerFocus &amp;&amp; !inputFocus\"\n    },\n    {\n      \"key\": \"ctrl+n\",\n      \"command\": \"explorer.newFile\",\n      \"when\": \"filesExplorerFocus\"\n    },\n    {\n      \"key\": \"ctrl+shift+n\",\n      \"command\": \"explorer.newFolder\",\n      \"when\": \"filesExplorerFocus\"\n    },\n    {\n      \"key\": \"r\",\n      \"command\": \"renameFile\",\n      \"when\": \"filesExplorerFocus &amp;&amp; !inputFocus\"\n    },\n    {\n      \"key\": \"delete\",\n      \"command\": \"deleteFile\",\n      \"when\": \"filesExplorerFocus &amp;&amp; !inputFocus\"\n    },\n    {\n      \"key\": \"ctrl+c\",\n      \"command\": \"filesExplorer.copy\",\n      \"when\": \"filesExplorerFocus &amp;&amp; !inputFocus\"\n    },\n    {\n      \"key\": \"ctrl+v\",\n      \"command\": \"filesExplorer.paste\",\n      \"when\": \"filesExplorerFocus &amp;&amp; !inputFocus\"\n    },\n    {\n      \"key\": \"ctrl+x\",\n      \"command\": \"filesExplorer.cut\",\n      \"when\": \"filesExplorerFocus &amp;&amp; !inputFocus\"\n    },\n    {\n      \"key\": \"space\",\n      \"command\": \"filesExplorer.openFilePreserveFocus\",\n      \"when\": \"filesExplorerFocus &amp;&amp; !inputFocus\"\n    },\n]\n</code></pre>","tags":["Setup"]},{"location":"blog/2025/new-device-setup/#hobbies","title":"Hobbies","text":"<ul> <li>Musescore</li> <li>Da Vinci Resolve</li> </ul> <ol> <li> <p>The only reason I am still using your OS and not Linux is the ability to use proprietary software\u00a0\u21a9</p> </li> </ol>","tags":["Setup"]},{"location":"blog/2025/note-taking/","title":"Obsidian - The last note taking app I'll ever use","text":"<p>I think I finally have it. The best way I would consider taking notes.</p>","tags":["Obsidian"]},{"location":"blog/2025/note-taking/#things-i-need","title":"Things I need","text":"<ol> <li>Sync notes across devices (and backup)</li> <li>Be able to write math<sup>1</sup> (Bye Workflowy<sup>2</sup>)</li> <li>Have privacy (I don't think any online solution gives me true ownership over my notes)</li> <li>Be able to publish pages online (Notion?)</li> <li>Be extremely fast (Bye Notion, and Logseq<sup>3</sup>)</li> <li>Works offline!</li> <li>Have a distraction free writing experience (no flashy stuff. Just pure minimalism)</li> <li>Let me use my favorite text editor (looking at you Vim)</li> <li>Let me switch context quickly without thinking too much.</li> <li>Allow markdown! (Bye MS Word and other proprietary software)</li> </ol> <p>Ok. I will stop right here. I think I could address all these with the mother of all note-taking apps:</p>","tags":["Obsidian"]},{"location":"blog/2025/note-taking/#why-obsidian","title":"Why Obsidian?","text":"<ul> <li> Obsidian Git plugin<ul> <li>Use <code>git</code> normally on my windows PC.</li> <li>Use <code>termux</code> on android and obsidian app to access content (read-only) on my phone. Utilizes fine-grained tokens so that I can access only that repository from my phone.</li> </ul> </li> <li> Obsidian \\(\\LaTeX\\) suite<ul> <li>Helps me write math really quick and also allows me to use snippets for boilerplate stuff that I need to add from time to time.</li> <li>Also has the option (which I haven't utilized) to use <code>javascript</code> code to generate text based on a snippet pattern and a parameter value</li> </ul> </li> <li> Privacy is default<ul> <li>I am not hosting my notes anywhere but on my private GitHub repository</li> </ul> </li> <li> I can actually publish very easily using obsidian<ul> <li>Initially I used Quartz but then I started using Mkdocs Material</li> <li>This gives me an added convenience to publish only a part of the vault while leaving the other part private for me. That's a lot of control.</li> <li>Also, I focus more on content (and actually can even forget that I have a blog online)</li> </ul> </li> <li> Extremely fast<ul> <li>Extremely fast</li> <li>Extremely fast</li> <li>Extremely fast</li> <li>Extremely fast</li> <li>Extremely fast</li> </ul> </li> <li> Cause it works offline.</li> <li> Distraction free?<ul> <li>Initially I was worried about graphs and how I wanted it to look. Later I just settle for the Prism theme.</li> <li>I use the dark/light mode in conjunction, whenever I need to do academic work, I use the light mode and the dark mode for writing stuff (like this blog). Helps me stop being derailed when I am doing important work (which I will do invariably on the light mode)</li> </ul> </li> <li> It supports Vim! That's a sale for me</li> <li> Switching context?<ul> <li>I use a plugin called <code>longform</code> which is actually used to write longform texts, but it helps me organize and access different folders quickly without needing to click on the native obsidian folders a lot.</li> </ul> </li> <li> Obsidian is all markdown</li> <li> It is boring</li> <li> It is limited</li> </ul>","tags":["Obsidian"]},{"location":"blog/2025/note-taking/#a-little-showcase","title":"A little Showcase","text":"<p>Obsidian on PC</p> <p></p> <p>Obsidian on mobile (Focusing)</p> <p></p> <p>Obsidian on mobile (Leisure reading)</p> <ol> <li> <p>I didn't learn \\(\\LaTeX\\) for nothing. I want to be able to use it cause its just so darn good!\u00a0\u21a9</p> </li> <li> <p>Only for notetaking, for general things that require syncing Workflowy is good\u00a0\u21a9</p> </li> <li> <p>Though I have to say, logseq was one of its kind. I really loved it. But Obsidian was just more mature.\u00a0\u21a9</p> </li> </ol>","tags":["Obsidian"]},{"location":"blog/2025/open-source-supremacy/","title":"Open-source Supremacy!","text":"<p>Ever switched to open-source software because you wanted to support a cause? I am not that type of person. I use whatever's faster and more convenient for me. And that is what led to a huge shock for me!</p> <p>I had been switching to apps as and when I found a better one, one that performs faster and has less-nonsense or more features that immediately want me to switch!</p> <p>Take a look at these apps that excel at what they do and work very well for me:</p> <code>Zotero</code> It is a reference manager, but I literally use it for keeping all my study pdfs for my actuarial exams and it does it extremely well. I also use it to store all the excel files that I create for each lesson or paper and that helps me just forget that I need to maintain or organize this file. <code>Obsidian</code> The one note-taking app that beats most of the others simply because its so fast and easy to work it. It gives you so many features without bloating that you don't feel obligated to use any feature till you really need it. And it just does the job! <code>Zen Browser</code> I was an arc fan... well... till I wasn't. Arc was the refresh I needed from chrome till I realized just how slow it performed. When did I realize it? When my friend Sam (whom I introduced to Arc btw!) sent me an article over the internet that Arc was a dead browser and Zen is the open-source alternative that everyone should try out. I was extremely skeptical thinking that its an open-source project. How can it be as good as Arc which has already gone so far? But I was wrong. Zen not only outperformed Arc, but I felt the difference in speed that I had just neglected because of the bias I had about Arc, giving me all these new features that were now hard to live without. <code>MuseScore Studio</code> Yes! This music making app that has been able to replace Finale for me (which I never thought it would be able to) has always been open source and that is the reason why it has been able to improve a lot over a very small span of years. <code>OBS Studio</code> OBS obviously! The recording software has been around for so long and I don't think so people realize how great this piece of software is. Firstly I don't really know any other method of broadcasting screens for code casts. Like do they make professional software for this kind of thing? I don't know, cause OBS really meets all the requirements that any professional setting might need. <code>Habitica</code> How can I forget this one!? I have been using so many todo list apps. You know the problem with those is that they come with their own profit making strategies. So they will deprive you of some most essential features. I gave up on them. But you know, having a todo-list is always a good thing so that you know its there when you need it. But Habitica goes beyond just being a todo-list. You can develop habits (as the name hints), and keep yourself disciplined about your daily activities. It uses the concept of gamification of life which IMHO is a very well thought out approach. I think I will write another post about this app later. I just love it so much (and didn't realize that it was actually open-source!) <p>Yeah! There are other apps that I have not been able to replace with open source software. But for a different reason. For example MS Excel has been around for so long that it has reached a point where developers are debating about the blink rate of the cursor to improve UX. They have surpassed the end of the spreadsheet game and are truly monopolies where other spreadsheet tools like Libre Calc are just there as \"alternatives to Excel\" but Excel stays the OG that might virtually never meet its competent opponent.</p>"},{"location":"blog/2025/tailwind-anywhere/","title":"Tailwind Anywhere!","text":"<p>The <code>tl;dr</code> is the tutoral. If you wanna know why you would want to do something like this, read Why would you wanna use this? section below.</p>","tags":["Webdev","Tailwind"]},{"location":"blog/2025/tailwind-anywhere/#tldr","title":"tl;dr","text":"<p>Source</p> <p>Follow this guide and select the \"Tailwind CLI\" method</p> <p>First up. Install the tailwindcss CLI.</p> <pre><code>pnpm install tailwindcss @tailwindcss/cli\n</code></pre> <p>Then create a file called <code>input.css</code>. Here, you will define the config as you do in a normal CSS file that is going to be using tailwindcss (e.g. think of it as the <code>global.css</code> in a Next.js project)</p> src/input.css<pre><code>@import \"tailwindcss\";\n</code></pre> <p>Run the cli with the <code>--watch</code> mode</p> <pre><code>pnpx @tailwindcss/cli -i ./src/input.css -o ./src/output.css --watch #(1)\n</code></pre> <ol> <li>The <code>--watch</code> mode ensures that whenever you make any edits to any of the html files in the directory which mention a tailwind compatible class name, that class and its css will be added to the <code>output.css</code> file.</li> </ol> <p>Then, import the <code>output.css</code> file in the <code>&lt;head/&gt;</code> of the html document being sourced.</p> <pre><code>&lt;head&gt;\n  &lt;!-- ... --&gt;\n  &lt;link href=\"./output.css\" rel=\"stylesheet\"&gt;\n  &lt;!-- ... --&gt;\n&lt;/head&gt;\n</code></pre> <p>As you keep updating the html file. <code>@tailwindcss/cli</code> will update the <code>output.css</code> file based on your configurations set in <code>input.css</code>. That's how God intended tailwindcss to work from the beginning. </p>","tags":["Webdev","Tailwind"]},{"location":"blog/2025/tailwind-anywhere/#why-would-you-wanna-use-this","title":"Why would you wanna use this?","text":"<p>Ever thought of spinning up a quick static webpage that doesn't need those flashy features of the cutting-edge frameworks that are being used literally everywhere? Well, I don't think so. But, if you are not a native-css person and tend to use tailwindcss just because its very convenient, I have good news for you.</p> <p>You can think of designing the page like how tailwind allows you to do (quickly and efficiently) without worrying about the native css styles. And the best part is that you don't need to think about installing tailwind as a dependency or bundling it or any of that crap (though if its a big project, its recommended, you learn how to do that to save time in the long run). But this isn't what this guide is talking about.</p> <p>We need to do some quick and dirty css work and fast. We don't need to do it the \"right\" or left way. We just want it to work!</p>","tags":["Webdev","Tailwind"]},{"location":"centre/","title":"Centre","text":""},{"location":"centre/#tag:math","title":"Math","text":"<ul> <li>            Math in `mdsvex`          </li> </ul>"},{"location":"centre/#tag:mind","title":"Mind","text":"<ul> <li>            The Mental Keyboard          </li> </ul>"},{"location":"centre/#tag:music","title":"Music","text":"<ul> <li>            Music Tools          </li> <li>            The Mental Keyboard          </li> </ul>"},{"location":"centre/#tag:obsidian","title":"Obsidian","text":"<ul> <li>            Obsidian - The last note taking app I'll ever use          </li> </ul>"},{"location":"centre/#tag:repost","title":"Repost","text":"<ul> <li>            How do I balance it?          </li> </ul>"},{"location":"centre/#tag:setup","title":"Setup","text":"<ul> <li>            How I set up my new Windows machine          </li> <li>            My new blogging workflow works extremely well!          </li> </ul>"},{"location":"centre/#tag:tailwind","title":"Tailwind","text":"<ul> <li>            Tailwind Anywhere!          </li> </ul>"},{"location":"centre/#tag:webdev","title":"Webdev","text":"<ul> <li>            Tailwind Anywhere!          </li> </ul>"},{"location":"centre/#tag:workflow","title":"Workflow","text":"<ul> <li>            My new blogging workflow works extremely well!          </li> </ul>"},{"location":"centre/academic/","title":"Academic","text":""},{"location":"centre/academic/#tag:exam","title":"Exam","text":"<ul> <li>            Chain Ladder Reserve Variance Estimation          </li> <li>            Exam 5          </li> <li>            Exam 7          </li> </ul>"},{"location":"centre/actuarial-exams/","title":"Actuarial Exams","text":""},{"location":"centre/actuarial-exams/#tag:five","title":"Five","text":"<ul> <li>            Adjustments to Data          </li> <li>            All about Data          </li> <li>            Basics          </li> <li>            Berquist-Sherman          </li> <li>            Bornhuetter-Ferguson          </li> <li>            Cape Cod          </li> <li>            Case Outstanding          </li> <li>            Chain Ladder / Dev          </li> <li>            Claims-made Ratemaking          </li> <li>            Classification          </li> <li>            Credibility          </li> <li>            Development          </li> <li>            Development Techniques          </li> <li>            Estimating ALAE          </li> <li>            Estimating ULAE          </li> <li>            Evaluation of Techniques          </li> <li>            Exam 5          </li> <li>            Expected Claims          </li> <li>            Frequency-Severity          </li> <li>            Implementation          </li> <li>            Indications vs Reality          </li> <li>            Individual Risk Rating          </li> <li>            Introduction          </li> <li>            Large Events &amp; Anamolies          </li> <li>            Lines of Business          </li> <li>            Multivariate Risk Classification          </li> <li>            On-levelling Data          </li> <li>            One time changes          </li> <li>            Other Considerations          </li> <li>            Overall Rate Indication          </li> <li>            Ratemaking          </li> <li>            Recoveries          </li> <li>            Reinsurance          </li> <li>            Reserving          </li> <li>            Salvage &amp; Subrogation          </li> <li>            Special Classification          </li> <li>            Strategy          </li> <li>            The Insurance Product          </li> <li>            Trends          </li> <li>            UW Expenses, LAE &amp; Profits          </li> <li>            Univariate Risk Classification          </li> </ul>"},{"location":"centre/actuarial-exams/#tag:seven","title":"Seven","text":"<ul> <li>            Chain Ladder Reserve Variance Estimation          </li> <li>            Exam 7          </li> </ul>"},{"location":"centre/tech/","title":"Techniques","text":""},{"location":"centre/tech/#tag:technique","title":"Technique","text":"<ul> <li>            Chain Ladder Reserve Variance Estimation          </li> <li>            Custom Classes in sklearn          </li> </ul>"},{"location":"exam-5/","title":"Ratemaking &amp; Reserving","text":"<p>Just so you know</p> <p>I am currently writing this entire guide/walkthrough of the exam. It's incomplete and full of errors but feel free to read through and suggest corrections.</p>","tags":["Five","Exam"]},{"location":"exam-5/#basics","title":"Basics","text":"<p>This section will provide foundational knowledge for understanding the intricacies of ratemaking and reserving in the insurance industry.</p>","tags":["Five","Exam"]},{"location":"exam-5/#the-insurance-product","title":"The Insurance Product","text":"<p>To establish context, we will begin by exploring the fundamental nature of the insurance product. This will involve a brief discussion of various lines of business such as:</p> <ul> <li>Property Insurance: Covering physical assets against perils like fire, theft, and natural disasters.</li> <li>Casualty Insurance: Protecting against liabilities arising from negligence or accidents, including auto liability and general liability.</li> <li>Life Insurance: Providing financial protection upon the death of the insured.</li> <li>Health Insurance: Covering medical expenses.</li> <li>Workers' Compensation: Providing benefits to employees for work-related injuries or illnesses.</li> </ul> <p>Understanding these different lines of business will highlight the diverse risks and considerations involved in insurance operations.</p>","tags":["Five","Exam"]},{"location":"exam-5/#all-about-data","title":"All About Data","text":"<p>Following the overview of insurance products, we will delve into the critical role of data. This segment will cover:</p> <ul> <li>Data Aggregation Techniques: Methods for collecting, compiling, and organizing vast amounts of insurance-related data. This could include discussing data sources (e.g., policy administration systems, claims systems, external data providers) and the importance of data quality.</li> <li>Related Calculations: An introduction to basic calculations performed on aggregated data, such as loss ratios, expense ratios, and combined ratios, to provide an initial understanding of key performance indicators in insurance.</li> </ul>","tags":["Five","Exam"]},{"location":"exam-5/#ratemaking","title":"Ratemaking","text":"<p>Ratemaking is the process of establishing the appropriate price for an insurance policy. This involves meticulous analysis of various data types, applying adjustments, and performing calculations while considering a multitude of factors to ensure solvency, competitiveness, and fairness.</p>","tags":["Five","Exam"]},{"location":"exam-5/#overall-ratemaking","title":"Overall Ratemaking","text":"<p>This section focuses on the comprehensive approach to setting rates for an insurance company's policies.</p> <p>We will learn about making crucial adjustments to raw data for ratemaking purposes:</p> <ol> <li>Presence of Large Events and Anomalies in Data: Techniques for identifying and mitigating the impact of catastrophic events (e.g., hurricanes, major lawsuits) or unusual data points that could skew rate indications. This might involve discussing capping, smoothing, or removing outliers.</li> <li>On-Level Data to Current Levels of Prices: Methods to adjust historical premium and loss data to reflect the current level of prices and rate structures. This ensures that past experience is comparable to the current pricing environment.</li> <li>Develop Claims Data to Ultimate Values: Understanding the concept of \"incurred but not reported\" (IBNR) losses and methods to project incomplete historical claims data to their ultimate expected payout. This is crucial for accurately reflecting the true cost of claims.</li> </ol> <p>We will also incorporate Underwriting Expenses and Profits into our calculations. This includes understanding different types of expenses (e.g., commissions, administrative costs) and how profit provisions are built into the rate to ensure the insurer's financial viability.</p> <p>Finally, we will learn how to set an overall rating indication using the processed and adjusted data. This involves combining all components (expected losses, expenses, profit) to arrive at a preliminary rate level.</p> <p>We will also touch on topics like \"Claims-made ratemaking\" and how they differ from occurrence policies' ratemaking, which we would have been assuming until now. This will highlight the distinct characteristics of these policy types and their implications for rate setting.</p>","tags":["Five","Exam"]},{"location":"exam-5/#classification","title":"Classification","text":"<p>Setting only an overall rate for all insureds can be inequitable and inefficient. This section will explore the importance of classification of insureds and the methodologies for pricing policies differently based on risk characteristics. We will look into the following techniques:</p> <ol> <li>Univariate Classification: Analyzing the impact of individual rating factors (e.g., age, geographic location, vehicle type) on losses in isolation.</li> <li>Multivariate Classification: Employing statistical techniques to analyze the combined impact of multiple rating factors simultaneously, leading to more refined and accurate risk differentiation (e.g., using GLMs).</li> <li>Special Classification: Discussing unique classification challenges or approaches that might not fit neatly into univariate or multivariate categories, such as professional liability classifications or specialized industry ratings.</li> </ol> <p>We will briefly touch on individual risk rating, where large customers are priced based on personalized information. This is relevant when the individual insured's data is sufficiently credible to warrant a tailored rate.</p>","tags":["Five","Exam"]},{"location":"exam-5/#next","title":"Next","text":"<p>Even after robust calculations, practical implementation requires further considerations.</p> <p>We also have to keep other considerations in mind. Not everything in life can be implemented as planned. Similarly, we have to make certain compromises based on regulations and many other reasons. Before we can use the results that we have obtained via our calculations, we have to keep:</p> <ol> <li>Credibility: Assessing the statistical reliability of the data used in ratemaking. This involves understanding how much weight to give to an insurer's own experience versus broader industry experience, especially for smaller companies or newer lines of business.</li> <li>Implementation Constraints: Recognizing the practical limitations and challenges in applying calculated rates, such as regulatory approvals, competitive pressures, market acceptance, and system limitations.</li> <li>Other Considerations: Broader factors influencing rate decisions, including legal precedents, social inflation, emerging risks, and the overall economic environment.</li> </ol> <p>in mind so that our rates can be practically implemented.</p>","tags":["Five","Exam"]},{"location":"exam-5/#reserving","title":"Reserving","text":"<p>Reserving in an insurance company is the process of estimating the financial obligations for future claim payments. It is crucial for financial stability, regulatory compliance, and accurate financial reporting.</p>","tags":["Five","Exam"]},{"location":"exam-5/#development-techniques","title":"Development Techniques","text":"<p>We will introduce why setting reserves in an insurance company is important and the process it involves. We need to develop our losses to get an estimate of the reserves that we need. In order to do so, we will use the following techniques:</p> <ol> <li>Chain Ladder Development: A widely used actuarial method for projecting ultimate losses based on historical loss development patterns.</li> <li>Expected Claims Method: A method that uses an expectation of ultimate losses, often based on exposure and expected loss ratios, particularly useful when historical data is limited.</li> <li>Bornhuetter-Ferguson Technique: A blend of the Chain Ladder and Expected Claims methods, combining actual loss experience with an a priori expectation of losses.</li> <li>Cape Cod Method: Another loss development technique that considers exposure and expected loss ratios in its projection.</li> <li>Frequency-Severity Method: Projecting ultimate losses by separately estimating the number of claims (frequency) and the average cost per claim (severity).</li> <li>Case Outstanding Method: Directly assessing the estimated cost of each open claim.</li> </ol>","tags":["Five","Exam"]},{"location":"exam-5/#adjustments-reviews","title":"Adjustments &amp; Reviews","text":"<p>We also might need to consider calendar year effects like changes in claims processing that can lead to an increase or decrease in the claims settlement rates or any change in the insurer's method for setting reserves. In such cases we use techniques like the Berquist Sherman</p> <p>Finally, we will close by Evaluation of Techniques where we discuss how we can review our reserves as time goes by to see if our reserves were set appropriately or not, which helps us make our reserves better than we had set before. This involves analyzing actual versus expected development and adjusting future reserving practices.</p>","tags":["Five","Exam"]},{"location":"exam-5/#the-delta","title":"The Delta","text":"<p>This section addresses important topics that, while not core to the direct ratemaking or reserving processes, are key considerations and significantly impact the accuracy and completeness of these activities.</p> <ol> <li>Recoveries like salvage and subrogation: Understanding how funds recovered from damaged property (salvage) or from responsible third parties (subrogation) impact the net cost of claims and should be accounted for in both pricing and reserving.</li> <li>Estimating ALAE and ULAE: Discussing the estimation of  Allocated Loss Adjustment Expenses (ALAE), which are expenses directly attributable to a specific claim (e.g., legal fees, adjustor fees), and Unallocated Loss Adjustment Expenses (ULAE), which are general claims handling expenses not tied to specific claims (e.g., claims department salaries, office rent). Both are crucial components of the overall cost of claims and must be accurately projected.</li> </ol>","tags":["Five","Exam"]},{"location":"exam-5/strategy/","title":"Strategy","text":"<ul> <li>I thought of doing the basic problem pack (once go through all questions) and then do the year-on-year questions. But I procrastinated. I think its better for me to do the YoY questions.</li> <li>While studying the material for the first time (after each section), look at the basic problem pack and check how the questions are.</li> </ul> <p>Verall talks about </p>","tags":["Five"]},{"location":"exam-5/0.basics/","title":"Basics","text":"","tags":["Five"]},{"location":"exam-5/0.basics/0.2.all-about-data/","title":"All about Data","text":"<p>In order to understand the different aggregation methods (CY, AY, PY), we will use a concrete example so that its crystal clear.</p>","tags":["Five"]},{"location":"exam-5/0.basics/0.2.all-about-data/#data","title":"Data","text":"Policy Effective Date Expiration Date Initial Policy Premium 1 June 1, 2012 May 31, 2013 480 2 July 1, 2012 December 31, 2012 125 3 March 1, 2013 February 28, 2014 225 4 August 1, 2013 March 31, 2014 300 <p>Also, six months after the policy expires, the initial policy premium on every policy increases by 8% due to the final audit.</p>","tags":["Five"]},{"location":"exam-5/0.basics/0.2.all-about-data/#calculate","title":"Calculate","text":"<ol> <li>CY 2013 EP a.o. Dec 31, 2013</li> <li>CY 2013 WP a.o. Dec 31, 2013</li> <li>PY 2013  EP a.o. Dec 31, 2013</li> <li>PY 2013 WP a.o. Dec 31, 2014</li> </ol>","tags":["Five"]},{"location":"exam-5/0.basics/0.2.all-about-data/#principles","title":"Principles","text":"<ul> <li>CY aggregation means to include all the transactions made in that given calendar year. <ul> <li>Meaning, it could be from policies which are not written/effective that year. And when I say \"all the transactions\", it includes any movement of money that happened in that year.</li> <li>Which means for Policy 2, the final audit happens on Jul 1, 2013 (six months after expiration, check table above) and so even though none of Policy 2 premium is earned in CY 2013, its audit amount has to be included in CY 2013 (which is \\(125 \\times 8\\% = 10\\)).</li> </ul> </li> <li>PY aggregation means to include the transactions of all the policies effective in that given policy year.<ul> <li>Meaning, we only look at the effective date of the policy, underlying the transaction and use that blindly to decide whether to include or exclude that transaction.</li> <li>If the policy reopens and has the same policy effective date, then also it has to be included in the original policy year.</li> </ul> </li> <li>WP means all the premium that was written in that particular aggregation (CY/PY).<ul> <li>You have to put the full term premium amount.</li> <li>Think of this story, you go to an UW and buy a policy. He bills you with some money, you pay it to him. He records/writes it in the company's policy database. That \"act\" has to be included in the written premium aggregation.</li> <li>This also means that if you go back and decide to cancel the policy at some later date, he will deduct the amount that you have already earned from that premium and return you some money (which gets \"unwritten\" from the company database). This \"act\" has to be recorded as unwriting of premium (negative written premium) in the database.</li> <li>Now, you think of WP in terms of CY or PY, just check whether the transaction of writing or unwriting premium has to be included in that CY/PY based on the definition of CY/PY and just add or subtract (in case of cancellation).</li> <li>A question: Should premium audits be included in WP? Of course! You can think of it as \"new money\" which needs to be added to that policy. Note that you never go back and edit the old records in the database, you create new transactions to audit.</li> </ul> </li> <li>EP refers to the amount for which coverage to the insured has already been provided.<ul> <li>To illustrate this, imagine you bought an annual policy today (6/19/2025) for $1000. The company is then obligated to provide you with coverage if something happens to you tomorrow (6/20/2025). It is also obligated to to provide you with coverage if something happens to you one day before the policy expires (6/18/2025). But there is a difference between the two dates.</li> <li>Tomorrow, if you decide to cancel the policy, the company is not obligated to provide you with coverage thereafter. However, it has to pay you almost the full amount since it effectively gave you coverage for only one day (today, virtually negligible).</li> <li>However, if you decide to cancel the policy one day before it expires, company will not provide coverage thereafter. BUT, it will only pay you a very little bit of money (the amount that it charged you for one day of coverage). Why? Because it provided you with coverage for the 364 days that has already passed. They have to charge you for it, because if anything would have happened to you in those 364 days, they really would have protected you. And so, charging you for those days makes sense.</li> <li>This is the concept of earning premium. How do we know how much we are paying for each day? The answer is however you assume it to be. If you decide that the premium should be earned uniformly (split all premium equally between all days of coverage) and just calculate a fraction of the total premium.</li> <li>So, if you understood correctly, EP for a PY will naturally contain the full premium amount as of the given date. In this case the only calculation you need to do is IF an \"as of (a.o.)\" date exists. (Note again that you should only include the policy if its effective date lies in the PY)</li> <li>Where as, EP for a CY will be calculated by taking the end point as the end of the calendar year. So just consider the end of the CY as the \"as of\" date.</li> </ul> </li> </ul>","tags":["Five"]},{"location":"exam-5/0.basics/0.2.all-about-data/#answers","title":"Answers","text":"<p>Now use the following principles to find out the answers to the above</p> <ol> <li>\\(\\$\\ 623.4\\)</li> <li>\\(\\$\\ 573.4\\)</li> <li>\\(\\$\\ 375\\)</li> <li>\\(\\$\\ 576\\)</li> </ol>","tags":["Five"]},{"location":"exam-5/0.basics/0.1.the-insurance-product/","title":"The Insurance Product","text":"<p>Before we start the discussion about the pricing and reserving, we must be aware about how insurance works.</p>","tags":["Five"]},{"location":"exam-5/0.basics/0.1.the-insurance-product/0.1.1.lines-of-business/","title":"Lines of Business","text":"","tags":["Five"]},{"location":"exam-5/1.ratemaking/","title":"Ratemaking","text":"","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.2.uw-expenses-lae-and-profits/","title":"UW Expenses, LAE &amp; Profits","text":"","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.3.overall-rate-indication/","title":"Overall Rate Indication","text":"","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.4.claims-made-ratemaking/","title":"Claims-made Ratemaking","text":"","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.6.individual-risk-rating/","title":"Individual Risk Rating","text":"","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.1.adjustments-to-data/","title":"Adjustments to Data","text":"","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.1.adjustments-to-data/1.1.1.adjust-for-anomalies/","title":"Large Events &amp; Anamolies","text":"<p>This section focuses on handling infrequent and high variance events, such as large losses or catastrophes. These events typically require more data to project expected losses accurately.</p>","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.1.adjustments-to-data/1.1.1.adjust-for-anomalies/#balancing-stability-with-responsiveness","title":"Balancing Stability with Responsiveness","text":"<p>The core actuarial theme here is balancing stability with responsiveness. Because these events don't occur frequently, looking at the latest 50 years of data might be necessary to provide a stable loss estimate. However, using data from 50 years ago might not be responsive enough, as conditions and relevance can change significantly over such a long period.</p>","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.1.adjustments-to-data/1.1.1.adjust-for-anomalies/#the-big-picture-estimating-future-losses-with-anomalies","title":"The Big Picture: Estimating Future Losses with Anomalies","text":"<p>We aim to estimate future losses using historical data. However, this data can contain anomalies:</p> <ul> <li>Large (Shock) Losses:<ul> <li>The definition varies by business.</li> <li>These are extremely large losses relative to the size of the business written (e.g., a $2M loss is significant for a $10M book of business, but less so for a $1B book).</li> </ul> </li> <li>Catastrophe:<ul> <li>According to ISO, a catastrophe involves $25M of industry losses from an event and a large number of claims.</li> </ul> </li> <li>Challenge of Large Losses (Example):<ul> <li>Consider a true scenario where most years have $0.5M in losses, but there's a 10% chance of $2M.</li> <li>If our recent three years all had $2M (or none did), we can't get a good idea of the true expected loss, leading to overestimation or underestimation.</li> </ul> </li> <li>What if we don't adjust for them?<ul> <li>We'll overestimate during \"unlucky\" years and underestimate during \"lucky\" years.</li> </ul> </li> <li>What's the Goal?<ul> <li>To get the best estimate of the expected value, as we can never predict when an earthquake will strike.</li> <li>Rates should cover these costs over long periods and shouldn't over-react to lucky or unlucky years.</li> </ul> </li> </ul>","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.1.adjustments-to-data/1.1.1.adjust-for-anomalies/#how-to-adjust-for-shock-losses","title":"How to Adjust for Shock Losses?","text":"<p>When adjusting for shock losses, consider the following:</p>","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.1.adjustments-to-data/1.1.1.adjust-for-anomalies/#common-options","title":"Common Options:","text":"<p>(Jargon: non-excess losses + excess losses = ground-up losses)</p> <ul> <li>Cap Losses at Basic Limit:<ul> <li>This approach helps in calculating an expected loss for losses under the basic limit.</li> <li>Rates for losses above the basic limit must be derived separately.</li> <li>Historical premiums also need adjustment to basic limit rates, reflecting what premiums would have been collected if all policies were written with basic limits.</li> </ul> </li> <li>Cap Losses and Apply an Excess Loss Loading Factor:<ul> <li>This is common for property coverages.</li> <li>Losses are capped at a specific amount.</li> <li>A factor is applied to non-excess losses to account for the excess losses or \"load for the excess loss.\"</li> </ul> </li> <li>Remove Ground-Up Shock Losses and Apply a Shock Loss Loading:<ul> <li>This method is less common.</li> <li>It requires defining a \"shock loss\" and removing it instead of capping it.</li> </ul> </li> </ul>","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.1.adjustments-to-data/1.1.1.adjust-for-anomalies/#challenge","title":"Challenge:","text":"<p>If using the last two methods, a shock loss threshold needs to be defined.</p>","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.1.adjustments-to-data/1.1.1.adjust-for-anomalies/#goals","title":"Goals:","text":"<ul> <li>Include as many non-excess/non-shock losses as possible (a lower cap includes fewer non-excess losses).</li> <li>Minimize the volatility of non-excess/non-shock losses (a higher cap increases loss volatility).</li> </ul>","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.1.adjustments-to-data/1.1.1.adjust-for-anomalies/#choices-for-the-cap","title":"Choices for the \"Cap\":","text":"<ul> <li>Basic Policy Limits (assuming policies have limits):<ul> <li>This doesn't work for Workers' Compensation, where the goal is full recovery and return to work.</li> </ul> </li> <li>Actuarial Judgment:<ul> <li>Ask: \"Above which level do the losses become volatile?\"</li> </ul> </li> <li>Percentile of the Size of Loss Distribution:<ul> <li>For example, sort all losses and cap them at the 99th percentile loss amount.</li> </ul> </li> <li>Loss as a Percent of Insured Value:<ul> <li>For example, cap all losses at 98% of the insured value (e.g., for a $500k home, cap any loss for that home at $490k).</li> <li>See also: What are the Choices for the \"Cap\"</li> </ul> </li> </ul>","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.1.adjustments-to-data/1.1.1.adjust-for-anomalies/#choices-for-the-excess-lossshock-loss-loading-factor","title":"Choices for the \"Excess-loss/Shock-loss Loading Factor\":","text":"<ul> <li>Long-Term Average of Excess/Non-Excess:<ul> <li>Apply a 20-30 year excess/non-excess loading factor to 3-year non-excess losses.</li> </ul> </li> <li>How many years of data?<ul> <li>Balance the stability of the average ratio with responsiveness to changes.</li> </ul> </li> <li>Account for changes in average severity over time?<ul> <li>Trended Excess-Losses / Trended Non-Excess Losses: Trend historical losses to future policy cost levels.</li> <li>Vary Cap by Year (indexed to year): Since each year will have a different cost level.</li> <li>See also: Account for inflation or changes in average severity over time (trends in the excess layer are greater than in the non-excess layer).</li> </ul> </li> <li>#datascience Fit a statistical distribution to loss data and run a simulation.</li> </ul>","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.1.adjustments-to-data/1.1.1.adjust-for-anomalies/#calculations-for-shock-losses","title":"Calculations for Shock Losses","text":"<p>The intuition here is to find a factor that, when multiplied by non-excess losses, gives the correct expected amount.</p>","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.1.adjustments-to-data/1.1.1.adjust-for-anomalies/#example-calculation-for-excess-loss-loading","title":"Example: Calculation for Excess Loss Loading","text":"<p>Here, a shock loss is defined as a single loss greater than $1,200,000.</p> <ul> <li>Sum across the total excess and total non-excess loss and take the ratio (weighted ratio).</li> <li>Alternatively, take a straight average of the \"Excess Ratio\" column.</li> <li>Multiply this Excess Loading Factor for expected excess losses.<ul> <li># of Excess Claims: Count claims exceeding $1,200,000.</li> <li>Ground-Up Excess Losses: Sum the size of those shock losses for each Accident Year (AY).</li> <li>Losses Excess of $120k: How much of the loss is considered excess? Sum for each AY.</li> <li>Non-Excess Losses: Total reported losses minus excess losses.</li> <li>Excess Ratio: Compute the ratio: Excess losses / Non-excess losses.</li> </ul> </li> </ul>","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.1.adjustments-to-data/1.1.1.adjust-for-anomalies/#example-calculation-for-removal-of-ground-up-shock-loss","title":"Example: Calculation for Removal of Ground-Up Shock Loss","text":"<ul> <li>Note the difference: This focuses on \"shock losses\" directly, not just \"excess losses.\"</li> <li>When you multiply the shock ratio by non-shock losses, you get an estimated value for the expected losses (which would include shocks).<ul> <li># of Shock Losses: Count claims exceeding $1,200,000.</li> <li>Ground-Up Shock Losses: Sum the size of those shock losses for each AY.</li> <li>Non-Shock Losses: Total reported losses minus shock losses.</li> <li>Shock Ratio: Compute the ratio: Ground-up shock losses / Non-shock losses.</li> </ul> </li> </ul>","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.1.adjustments-to-data/1.1.1.adjust-for-anomalies/#how-to-adjust-for-catastrophic-losses","title":"How to Adjust for Catastrophic Losses?","text":"<p>Catastrophic losses are treated like \"shock\" losses and adjusted for on a ground-up basis, rather than as \"excess\" losses.</p>","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.1.adjustments-to-data/1.1.1.adjust-for-anomalies/#approach","title":"Approach:","text":"<p>Remove all catastrophic losses from the data and replace them with a Catastrophe Loading Factor (Cat LF).</p>","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.1.adjustments-to-data/1.1.1.adjust-for-anomalies/#two-components","title":"Two Components:","text":"<ul> <li>Modelled (infrequent):<ul> <li>Computer simulations where the insurer uploads their current book of business.</li> <li>These models generate loss distributions and expected losses for events where historical data is insufficient (e.g., hurricanes). The model might simulate a hurricane's impact on insured properties based on geographical outcomes to inform the dataset.</li> </ul> </li> <li>Non-modelled #tip (this is the only one tested):<ul> <li>This approach doesn't use a catastrophe model and covers events that occur relatively frequently (e.g., hailstorms).</li> <li>Long-term 20-30 year averages are often sufficient.</li> <li>Consider exposure growth in catastrophe-prone areas: If a previously less populated area prone to hailstorms has seen increased population, the same magnitude hailstorm could cause significantly more damage now.</li> <li>Relevance of old data: If building codes have changed, older data is less relevant. Always balance stability with responsiveness (relevance).</li> </ul> </li> </ul>","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.1.adjustments-to-data/1.1.1.adjust-for-anomalies/#goal","title":"Goal:","text":"<p>To establish a rate that, over 50 years, will cover losses from a 1-in-50-year earthquake, rather than attempting to predict the earthquake and increasing rates only for affected insureds in that specific year (which would be impractical).</p>","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.1.adjustments-to-data/1.1.1.adjust-for-anomalies/#non-pricing-measures","title":"Non-Pricing Measures:","text":"<ul> <li>Limit catastrophe risks:<ul> <li>Restrict writings in high-risk areas.</li> <li>Require high deductibles in high-risk areas (e.g., 10% of building value, so a $500k house has a $100k deductible).</li> </ul> </li> </ul>","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.1.adjustments-to-data/1.1.1.adjust-for-anomalies/#calculation-for-non-modelled-catastrophic-losses","title":"Calculation for Non-Modelled Catastrophic Losses","text":"<p>When calculating for non-modelled catastrophic losses:</p> <ul> <li>20 years of data is generally considered sufficient.</li> <li>Consider the Amount of Insurance.</li> <li>Take the ratio of Cat-to-AIY (then take the straight average):<ul> <li>Since losses and Amount of Insurance Year (AIY) will both change consistently due to inflation, taking the Cat-to-AIY ratio for each year ensures that the inflation factor cancels out. This ratio will be comparable across different years, as both the numerator and denominator will be at consistent inflationary levels.</li> </ul> </li> <li>Consider the ULAE factor (Unallocated Loss Adjustment Expenses / Loss &amp; Allocated Loss Adjustment Expenses only).</li> <li>Multiply with the future Average AIY per exposure in the Effective Period.</li> </ul>","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.1.adjustments-to-data/1.1.1.adjust-for-anomalies/#expense-anomalies","title":"Expense Anomalies","text":"<p>We've discussed loss data, but what about other types of data like premiums or expenses?</p> <ul> <li>Example: An unusually large expense for an insurer, such as buying a new computer system or software.</li> </ul>","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.1.adjustments-to-data/1.1.1.adjust-for-anomalies/#what-to-do","title":"What to do?","text":"<ul> <li>While accounting rules dictate financial reporting, actuaries can make their own assumptions for pricing.</li> </ul>","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.1.adjustments-to-data/1.1.1.adjust-for-anomalies/#approach_1","title":"Approach:","text":"<ul> <li>Smooth out like shock losses.</li> <li>Don't price for this at all: Assume it's paid from surplus, like a sunk cost.</li> </ul>","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.1.adjustments-to-data/1.1.1.adjust-for-anomalies/#consideration","title":"Consideration:","text":"<ul> <li>Consider the nature of the expense. Ask: Should future policyholders bear this cost?</li> </ul>","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.1.adjustments-to-data/1.1.3.development/","title":"Development","text":"","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.1.adjustments-to-data/1.1.3.development/#reasons-for-changes-in-loss-triangles","title":"Reasons for changes in Loss Triangles","text":"<ul> <li>Case Reserve Adequacy</li> <li>Settlement Rates</li> <li>Mix of business</li> <li>Growing / Shrinking book of business</li> <li>Tort Reform</li> <li>Occurrence of one or more large claims</li> </ul>","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.1.adjustments-to-data/1.1.3.development/#chain-ladder-method","title":"Chain Ladder Method","text":"","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.1.adjustments-to-data/1.1.3.development/#assumptions","title":"Assumptions","text":"<ol> <li>Future claims development will be similar to development in prior periods.<ul> <li>The same factors (derived from past development periods) are used in future development periods.</li> </ul> </li> <li>Claims observed for an immature period tell you something about claims yet to be observed.<ul> <li>The last row is used in calculation (as the base for the factors to be multiplied with).</li> </ul> </li> </ol>","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.1.adjustments-to-data/1.1.3.development/#impact-of-changes","title":"Impact of Changes","text":"<ul> <li> <p>Case Reserve Adequacy</p> <ul> <li>Setting Higher/Lower case reserves starting from a Calendar Year (CY).</li> <li>Reported Claims Method is NOT affected by changes in Loss Ratios (LR), but only by changes in case reserves.<ul> <li>NOTE: Shaded in ==gray== are new claims.</li> <li>If we set higher case reserves starting from Accident Year (AY) 2014 (i.e., $165,000), this impacts new claims only.</li> <li>Here, when we calculate the Cumulative Development Factor (CDF) from the prior periods (2 x 1.25 x 1.08 x 1.019 = 2.750 for 12-ultimate), we get ==$453,750==, which is a big overestimation!</li> <li>If we set higher case reserves starting in AY 2012 (i.e., $165,000), this affects new and existing claims. In this case too, we will end up overestimating ultimate losses for each of the AYs 2011-2014.</li> </ul> </li> </ul> </li> <li> <p>Settlement Rates</p> <ul> <li>Speedup (Slowdown) leads to Overestimation (Underestimation).</li> <li>Reason (for Speedup-overestimation): Historical dollar values for each period increased gradually. So, they gave CDFs which were bigger in value compared to if there was a speedup in settlement rates. Due to a speedup, the initial claims for the starting of an AY will be larger, thus, the bigger CDF and the larger initial claim paid will explode the ultimate value (overestimation).</li> <li>Loss / Paid Claims Method is NOT affected by changes in Case Reserve Adequacy (CRA), but only by changes in settlement rates.</li> <li>Wording<ul> <li>...would overestimate since it would apply historical development patterns that were based on less adequate Case Reserves to the now higher reported claims.</li> </ul> </li> </ul> </li> </ul>","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.1.adjustments-to-data/1.1.2.on-levelling-data/","title":"On-levelling Data","text":"","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.1.adjustments-to-data/1.1.2.on-levelling-data/1.2.2.one-time-changes/","title":"One time changes","text":"<p>One-time changes occur on a specific date or time. If predictions differ from targets, rates need adjustment.</p> <p>Action:</p> <ul> <li>Calculate direct effects.</li> <li>Adjust premiums for rate changes.</li> </ul>","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.1.adjustments-to-data/1.1.2.on-levelling-data/1.2.2.one-time-changes/#reasons","title":"Reasons","text":"<ul> <li>Rate Changes</li> <li>Law Changes<ul> <li>Implemented like a rate change or impact all policies (even in-force policies, whose premiums need recalculation).</li> <li>Changes to rates.</li> <li>Changes to coverages.</li> </ul> </li> <li>Court Rulings<ul> <li>These are imposed by a court decision instead of a law.</li> </ul> </li> </ul>","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.1.adjustments-to-data/1.1.2.on-levelling-data/1.2.2.one-time-changes/#effects","title":"Effects","text":"<p>Situation: A large rate increase occurs.</p> <ul> <li>Direct Effects:<ul> <li>The direct result is that premiums increase.</li> </ul> </li> <li>Indirect Effects:<ul> <li>These cause a change in customer behavior.</li> <li>Result: Lower retention and close ratios.</li> <li>These are often difficult to quantify.</li> <li>More examples:<ul> <li>Workers' compensation: An increase in indemnity benefits leads to more claims being filed and longer periods out of work.</li> <li>Rate decrease: Leads to an increase in retention and close ratios.</li> <li>Workers' compensation: A decrease in benefits leads to fewer injured workers filing claims and injured workers returning to work sooner.</li> </ul> </li> </ul> </li> </ul>","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.1.adjustments-to-data/1.1.2.on-levelling-data/1.2.2.one-time-changes/#calculating-direct-effects-on-premiums","title":"Calculating Direct Effects on Premiums","text":"<p>Both methods below yield the same output.</p> <ul> <li>Re-rate Each Policy Directly<ul> <li>Calculate <code>(Total after) / (Total before) - 1</code>.</li> </ul> </li> <li>Use In-Force Premium Distributions<ul> <li>Calculate the distribution: Determine the original premiums for each class (e.g., by territory A and B, where the distribution might be 52.6% and 47.4% respectively).</li> <li>Calculate the percentage change in the multiplicative factors: <code>(Final factor / Initial factor) - 1</code>. For example, <code>1.0/1.0 - 1.0 = 0%</code> and <code>0.85/0.90 - 1 = -5.56%</code>. Do this for all changes.</li> <li>Then, perform a sum-product: Calculate the average percentage change weighted by the distribution of premiums. Use this to determine the multiplicative premium change.</li> <li>Now, weight this multiplicative effect (e.g., 7.11%) and any additive effect (e.g., 0%) by the proportion of multiplicative (e.g., 3800/4000) and additive (e.g., 200/4000) components of the original premium.</li> </ul> </li> </ul>","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.1.adjustments-to-data/1.1.2.on-levelling-data/1.2.2.one-time-changes/#calculating-direct-effects-on-losses","title":"Calculating Direct Effects on Losses","text":"<ul> <li> <p>Change in Coverage</p> <ul> <li>Coverage Increase:<ol> <li>Losses might be capped at a lower coverage level (censored data), e.g., $500k versus an increased $1000k.</li> <li>A new type of coverage might not have historical data.</li> </ol> </li> <li>Coverage Decrease:<ul> <li>We can restate historical data at new coverage levels.</li> </ul> </li> <li> <p>Ways to Calculate</p> </li> <li> <p>Restate Individual Claims:</p> <ul> <li>Analogous to calculating premium levels at in-force policy levels. (e.g., Workers Compensation Indemnity Benefit Change).</li> </ul> </li> <li>Representative Groups:<ul> <li>Use a weighted average of the impact on different segments.</li> </ul> </li> <li>Simulate Losses Under New Coverage Levels:<ul> <li>This can be difficult due to parameter definition.</li> </ul> </li> <li> <p>Workers' Compensation Indemnity Benefit Change</p> </li> <li> <p>An example to calculate benefit change based on totals.</p> </li> <li>SAWW (State Average Weekly Wage): The minimum or maximum benefit can be stated as a percentage of the SAWW (e.g., Min: 50% of SAWW ($1000) = $500; Max: 100% of SAWW = $1000).</li> <li>See also: Restate individual claims (analogous to calculating premium levels at in-force policy levels).</li> <li>Rate: Stated as \"rate of 1/2 of their pre-injury wage.\" Here, 1/2 is the Compensation Rate.</li> <li>Situation:<ul> <li>Compensation rate changes from 1/2 to 2/3.</li> <li>Minimum benefit remains 50% of SAWW (no change).</li> <li>Maximum benefit changes from no limit to 100% of SAWW.</li> </ul> </li> <li>Calculation using Max Benefit (avg wage, and avg wage/SAWW):<ul> <li>Which rows will receive the minimum benefit?<ul> <li>Old rate: If (Weekly wage / SAWW) is less than 50% / (1/2) = 100%. So, the first three rows with a ratio to SAWW &lt; 100% will receive the minimum benefit as per the old rate.</li> <li>New rate: Do the same for the new rate with 50% / (2/3) = 75%. So, the first two rows with a ratio to SAWW &lt; 75% will receive a minimum benefit of $500 as per the new rate. This is natural, because if the weekly wage &lt; SAWW, wages are lower and need to be provided for, and vice versa.</li> </ul> </li> <li>Which rows will receive the maximum benefit?<ul> <li>If the ratio (1) is more than 100% / (2/3) = 150%. Only records &gt; 150% (the last one) will receive a maximum benefit of $1000.</li> </ul> </li> <li>The average weekly benefit is <code>MIN[$1000, MAX($500, $900 x 2/3)]</code>.</li> </ul> </li> <li>Direct Benefit change: <code>New benefits / Old benefits - 1</code>.</li> </ul> </li> </ul>","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.1.adjustments-to-data/1.1.2.on-levelling-data/1.2.2.one-time-changes/#adjust-premiums-for-rate-change","title":"Adjust Premiums for Rate Change","text":"<p>We can on-level any premiums, but this is usually done for Earned Premiums.</p> <ul> <li> <p>Extension of Exposures</p> <ul> <li>Re-rate all historical policies at an individual policy level with the new rates to get on-level full-term premiums, then recalculate Earned Premium (EP) and % earned using those rates.</li> <li>Advantage: Most accurate method.</li> <li>Disadvantage:<ul> <li>Requires detailed historical policy-level data (though nowadays this method is quite common as data is often available).</li> <li>Requires significant computing power for large datasets.</li> <li>Needs assumptions for new rating variables with no historical data.</li> <li>Difficult to incorporate changes in schedule rating guidelines for commercial lines.</li> </ul> </li> <li>Method: Simply calculate the premiums with the new rates.</li> <li> <p>Parallelogram Method</p> </li> <li> <p>Advantage: Quick to calculate.</p> </li> <li>Disadvantage:<ul> <li>Assumes policies are written evenly throughout the year. This can be partially addressed by using smaller time periods (like quarterly).</li> <li>Inappropriate for class ratemaking. The direct effects of a rate change are calculated at an aggregate level. So, if classes A and B have direct effects of 0% and 20% respectively, but the aggregate increased by 10%, the parallelogram method would apply a 10% direct effect to both A and B. This can be addressed by on-leveling premiums at the class level using class-level rate impacts and performing the method for each class.</li> <li>See also: Disadvantage #2 (class ratemaking) when done at a book of business level.</li> </ul> </li> <li>Method:<ul> <li>Gather effective dates and amounts of all rate changes.</li> <li>Group policies into rate level groups (RLG). RLGs are the triangular, parallelogram, or square portions in the policy diagram where each shape has the same rate level.</li> <li>Calculate the portion of the time period's earned exposure for each RLG. Just take a fraction of the shape's area over the year.</li> <li>Calculate the Cumulative Rate Level Index (CRLI) for each RLG. Assign the oldest RLG a rate level of 1 (so, CRLI = 1); subsequent ones can be calculated by applying the rate changes to the previous CRLI. For example, <code>CRLI(A) = 1</code> and <code>CRLI(B) = CRLI(A) x (rate change for B = 1.1) = 1 x 1.1 = 1.1</code>.</li> <li>Calculate the Weighted Average CRLI (WA.CRLI) for each time period. Areas in the diagram are the weights. For example, Area A = 0.5 x (6 months / 12 months) = 0.125 is the weight for RLG A, and Area B = 1 - Area A = 0.875 is the weight for RLG B. So, for this time period, the WA.CRLI = 0.125 x (1) + 0.875 x (1.1) = 1.0875. The average rate level for CY2011 would be 1.0875.</li> <li>Calculate the On-Level Factor (OLF) for each time period: <code>Current CRLI / WA.CRLI</code>. Say the latest CRLI calculated = 1.078. For CY2011, OLF = 1.078 / 1.0875 = 0.9913.</li> <li>Multiply OLF by the earned premium for the appropriate time period to obtain On-Level Earned Premium (OLEP). Say, the CY2011 EP = $50,000. Then the CY2011 OLEP = $50,000 x 0.9913 = $49,563.</li> <li>Interpretation: If historical policies had all been written with the current rates, we estimate CY2011 EP would have been $49,563 instead of $50,000.</li> </ul> </li> <li>Notes (B.3.6):<ul> <li>The slant of the rate change line depends on the policy term (6-month policies will have less slope than 1-year policies).</li> <li>We can ignore rate changes before the start of the time period we are considering for calculation. For example, if there was a rate change before CY2011 that has already affected all policies earning in 2011, we can just set the rate at the beginning of 2011 as CRLI = 1. This is because in the final step for calculating OLFs, we would anyway be canceling out the previous rate changes in the numerator and denominator.</li> <li>Law changes are represented by vertical lines, as they affect all in-force policies.</li> <li>For policy years, the time periods would now look like parallelograms; nothing else changes.</li> <li>Calendar quarters are just thinner time periods.</li> <li>Drawing diagrams can be helpful.</li> <li>Common Student Mistake: Don't calculate an OLF for each historical RLG. Do calculate a single average rate level from the historical period and obtain a single OLF for that period.</li> </ul> </li> <li>See also: Parallelogram Method (assumption of uniformity doesn't hold, so use shorter time periods), Parallelogram Method for losses, and Method (same as Parallelogram Method with the exception of using actual exposures instead of area).</li> <li> <p>Use Actual Writings Distribution and Group Data by Rate Level</p> </li> <li> <p>Advantage: More appropriate than the parallelogram method when actual writings are not uniform.</p> </li> <li>Disadvantage: Same as Disadvantage #2 (class ratemaking) when done at a book of business level.</li> <li>Method: Same as Parallelogram Method with one exception: instead of taking area (as a result of the uniformity assumption), consider the actual exposures (e.g., 10% in the first 2 months, 50% in the following 6, and 40% in the rest).</li> <li> <p>More...</p> </li> <li> <p>On-Level Written Premium (OLWP) with Law Change B.3.8</p> </li> <li>Parallelogram Method for losses.</li> </ul> </li> </ul>","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.1.adjustments-to-data/1.1.2.on-levelling-data/1.2.2.one-time-changes/#adjust-losses-for-coverage-changes","title":"Adjust Losses for Coverage Changes","text":"<p>NCCI estimates benefit changes. We assume that benefit changes impact all losses from all policies.</p> <ul> <li>Parallelogram Method<ul> <li>The assumption of uniformity doesn't hold well, so use shorter time periods.</li> </ul> </li> <li>Ways in which Benefit can Affect:<ul> <li>Slant lines: Losses on policies written after a certain date.</li> <li>Vertical lines: All new losses occurring after a certain date.</li> </ul> </li> </ul>","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.1.adjustments-to-data/1.1.2.on-levelling-data/1.2.3.trends/","title":"Trends","text":"<p>Trends represent changes in the mix of business (MoB), inflation, and socio-economic factors. These affect Premiums, Losses, Exposures, or Expenses over time.</p> <p>Note: Trends are not applied to the number of policies written, which also changes over time. The reason is that we want the MoB, inflation, and socio-economic conditions of the data to match those of the policy period being priced. The goal is to ensure the insurer hits their Target profit. We are not concerned about how many policies the insurer writes in the future, but rather, forecast the expected average costs and average premiums.</p>","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.1.adjustments-to-data/1.1.2.on-levelling-data/1.2.3.trends/#examples-of-trends","title":"Examples of Trends","text":"<p>Trends reflect changes that occur gradually over time:</p> <ul> <li>Gas Prices<ul> <li>Gas prices increasing may cause people to drive less, thus reducing the frequency of accidents.</li> </ul> </li> <li>Medical Care Costs<ul> <li>An increase in the cost of medical care can increase the claim severity of liability coverage.</li> <li>The insurer's rates become more and more inadequate over time if the policies are priced based on some older (lower) level of severity.</li> </ul> </li> <li>Higher Deductibles<ul> <li>Customers switching to higher deductibles will lead to a reduction in claims, and so they would be asked to pay less pure premiums.</li> <li>Fewer claims would cross the deductible threshold, so claim frequency reduces.</li> <li>There is lesser severity per claim as each claim is being deducted by a higher amount.</li> </ul> </li> <li>Labor and Material Costs<ul> <li>Linked to the labor and material costs, which may change over time.</li> <li>Since coverage for homeowners insurance is measured in terms of replacement cost, the cost of rebuilding the house increases due to the increase in material and labor costs.</li> <li>Average premiums increase. But since coverage is also increased, there will be an increase in the pure premium.</li> </ul> </li> <li>Payroll (Exposures)<ul> <li>When exposures are inflation sensitive, e.g., payroll in Workers' Compensation.</li> <li>Employers increase payroll to keep up with inflation. So the indemnity claim severity will increase as workers are paid a portion of their wages when they are out of work.</li> <li>Frequency per unit dollar of income reduces because the expected worker injury counts have nothing to do with increased payroll or inflation.</li> <li>We have to take a call based on the combined impact.</li> </ul> </li> </ul>","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.1.adjustments-to-data/1.1.2.on-levelling-data/1.2.3.trends/#asop-13","title":"ASOP 13","text":"<p>Read through 1-4, but 3 is most relevant for trending.</p> <ul> <li>Sect. 2.6 ASOP 13 applies to any type of data<ul> <li>Not just Prem/losses... Coverage, payroll, expenses.</li> </ul> </li> <li>Sect. 3.1 How to present?<ul> <li>Depending on the intended purpose, you may have to calculate a single point or a range of possible trends.</li> </ul> </li> <li>Sect. 3.2 Data<ul> <li>Based on insurance / non-insurance data?</li> <li>Credibility concerns?</li> <li>Relationship between data? (premium link with inflation).</li> <li>Distortions in data? (Seasonality).</li> </ul> </li> <li>Sect. 3.3 Relevant economic and social influences<ul> <li>Court decisions. Are courts being more generous to plaintiffs? If so, losses can go up.</li> </ul> </li> <li>Sect. 3.4 Methodology<ul> <li>Consider current best practices.</li> <li>Consider past methods used (no need to use them, just try them out).</li> <li>We need the best information (estimate) and to make the best business decision. Don't try to reverse engineer a trend rate to get a target rate changes.</li> </ul> </li> </ul>","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.1.adjustments-to-data/1.1.2.on-levelling-data/1.2.3.trends/#data-for-trending","title":"Data for Trending","text":"","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.1.adjustments-to-data/1.1.2.on-levelling-data/1.2.3.trends/#source","title":"Source","text":"<ul> <li>Insurer's data<ul> <li>Past trends would indicate future trends.</li> </ul> </li> <li>Industry data<ul> <li>More stable. But may be less relevant.</li> </ul> </li> <li>Economic data<ul> <li>Inflation. CPI (medical care).</li> </ul> </li> <li>Use one of them or a weighting of a combination.</li> </ul>","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.1.adjustments-to-data/1.1.2.on-levelling-data/1.2.3.trends/#distortions","title":"Distortions","text":"<ul> <li>One-time changes<ul> <li>Rate changes can cause change in average premiums. Can distort the trend for premiums.</li> </ul> </li> <li>Catastrophic or shock losses<ul> <li>Usually remove the record.</li> </ul> </li> <li>Seasonality<ul> <li>Annualize data in order to remove seasonality.</li> </ul> </li> </ul>","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.1.adjustments-to-data/1.1.2.on-levelling-data/1.2.3.trends/#concepts","title":"Concepts","text":"<ul> <li>Trends are based on difference in AVERAGE / pure premiums, not TOTAL premiums<ul> <li>Even if the number of policies written increases, the average/pure premium won't change.</li> <li>The same set of policies in the past (if written in the future) since it's the same set of policies it will be the same in number.</li> </ul> </li> <li>Granularity of data<ul> <li>Use state-level data for trends if we are looking at rate changes in a state. But if state-level data is volatile (small volume of data), consider using country-wide data.</li> <li>Homeowners, look at trends by peril. Fire-related vs. water-related. Then there is more homogeneity in the data. But not at the cost of statistical significance (volume of data).</li> </ul> </li> <li>Balance stability with responsiveness<ul> <li>Be conservative: it was a fluke and the change will not continue forward in the future. Select the historically stable rate.</li> <li>Be Fully responsive: the change is highly indicative of the rates in the future. Hence select the new rate.</li> <li>Be Judgemental: Somewhere in between.</li> </ul> </li> </ul>","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.1.adjustments-to-data/1.1.2.on-levelling-data/1.2.3.trends/#premium-data","title":"Premium data","text":"<ul> <li>Earned Premium<ul> <li>When forecasting Loss Ratios. EP is in the denominator of LR.</li> </ul> </li> <li>Written Premium<ul> <li>Is a leading indicator of EP.</li> </ul> </li> <li>Premium developments from audit #explore</li> </ul>","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.1.adjustments-to-data/1.1.2.on-levelling-data/1.2.3.trends/#loss-data","title":"Loss data","text":"<ul> <li>Trend frequency and severity separately<ul> <li>Re-combine freq/severity to PP trend.</li> </ul> </li> <li>CY data<ul> <li>For short tailed LOB.</li> <li>Assumes that book is not significantly growing.</li> </ul> </li> </ul>","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.5.classification/","title":"Classification","text":"<p>Grouping of individuals with similar risk profiles.</p>","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.5.classification/#asop-12","title":"ASOP 12","text":"","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.5.classification/#adverse-selection","title":"Adverse Selection","text":"Insured Type True Expected # of Risks Insurer A Price Profit # of Risks Insurer B Price Profit Low-risk $200 50 $500 $15,000 50 $500 $15,000 High-risk $800 50 $500 ($15,000) 50 $500 ($15,000) Total $500 100 $500 $0 100 $500 $0 <p>Info</p> <p>A identifies the distinction, charges Low-risk less (20 Low Risk individuals shift from <code>B</code> to <code>A</code>) charges High-risk more (20 High Risk individuals shift from <code>A</code> to <code>B</code>)</p> Insured Type True Expected Risks Insurer A Price Profit # Risks Insurer B Price Profit Low-risk $200 70 $400 $14,000 30 $500 $9,000 High-risk $800 30 $800 $0 70 $500 ($21,000) Total $500 100 $520 $14,000 100 $500 ($12,000) <ul> <li><code>A</code> is experiencing favorable selection.</li> <li><code>B</code> is experiencing adverse selection.</li> <li>Recognizing the lower cost group of insureds that is not identified by the competition, and distinguishing this through underwriting and marketing instead of just rating, is known as \"skimming the cream\" #jargon.</li> <li>Insurers use Underwriting (UW) Guidelines to protect this trade-secret.</li> </ul>","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.5.classification/#4-criteria-for-evaluating-a-rating-variable","title":"4 Criteria for Evaluating a Rating Variable","text":"","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.5.classification/#statistical","title":"Statistical","text":"<ul> <li>Statistical Significance<ul> <li>Expected Costs should vary by class with statistical significance.</li> </ul> </li> <li>Homogeneity<ul> <li>There should be no significantly different subclasses (otherwise they should be moved to their own separate class).</li> </ul> </li> <li>Credibility<ul> <li>The classes should be large enough or stable to allow credible statistical predictions.</li> </ul> </li> </ul>","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.5.classification/#operational-criteria","title":"Operational Criteria","text":"<ul> <li>Objective<ul> <li>Measurable and clearly defined (anti-example: psychological profile).</li> <li>Exhaustive and mutually exclusive.</li> </ul> </li> <li>Inexpensive to administer<ul> <li>Cost of obtaining and maintaining data to establish classes should not be too high.</li> </ul> </li> <li>Verifiable Levels<ul> <li>The levels of a rating variable (e.g., Short, Medium &amp; Tall) should be easily verifiable. We should minimize the ability for intentional misclassification.</li> <li>Miles driven: Telematics (verifiable) versus Insured reporting (hard to verify).</li> </ul> </li> </ul>","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.5.classification/#social-criteria","title":"Social Criteria","text":"<ul> <li>Affordability<ul> <li>Subsidies exist to promote affordability.</li> </ul> </li> <li>Causality (not a necessity)<ul> <li>The public will accept it as a variable if an intuitive cause and effect relationship can be shown.</li> </ul> </li> <li>Controllability (not a necessity)<ul> <li>To reduce hazards.</li> </ul> </li> <li>Privacy<ul> <li>Insured shouldn't feel a breach of privacy.</li> </ul> </li> </ul>","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.5.classification/#legal-criteria","title":"Legal Criteria","text":"<ul> <li>Must be in compliance with any laws in the jurisdiction in which the policy is being written.</li> </ul>","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.5.classification/1.5.1.univariate/","title":"Univariate Risk Classification","text":"<p>Pricing classes within the book.</p> <pre><code>Goal: Estimate the relative costs between classes\n</code></pre> <ul> <li>Univariate: Pricing one variable at a time.</li> <li>Assume: Rating algorithm is multiplicative. Goal is to get factors.<ul> <li>Base level is the one with relativity = 1.</li> <li>When calculating New indicated relativities, try keeping the base relativity as 1.</li> </ul> </li> </ul>","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.5.classification/1.5.1.univariate/#approaches","title":"Approaches","text":"<ul> <li> <p>Pure Premium</p> <ul> <li>Gets Indicated Relativities.</li> <li> </li> <li>Adjusted Pure Premium Method<ul> <li>See also: Best (=) adjusted exposures as in Adjusted Pure Premium Method</li> </ul> </li> <li> <p>Loss Ratio</p> </li> <li> <p>Gets Adjustment Factors.</p> </li> </ul> </li> </ul>","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.5.classification/1.5.1.univariate/#assumptions-no-exposure-correlation-between-rating-variable","title":"assumptions No exposure correlation between rating variable.","text":"<ul> <li>Also known as Distributional Bias.</li> <li>If Territory B has a high percentage of youthful drivers, we might misclassify Territory B as having higher risk, but the real issue is the youthful drivers.</li> </ul>","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.5.classification/1.5.2.multivariate/","title":"Multivariate Risk Classification","text":"<p>Minimum Biases Procedures, GLM Basics, Diagnostics, Data Mining Techniques.</p>","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.5.classification/1.5.2.multivariate/#benefits-of-using-multivariate-risk-classification","title":"Benefits of Using Multivariate Risk Classification","text":"<ul> <li>Focus on signal (not noise).</li> <li>Provide statistical Diagnostics.</li> <li>Consider for exposure correlation.<ul> <li>Exposure correlation: 75% of 16-year-old drivers are male, but 50% of 50-year-old drivers are male. With age, we can infer something about the sex of the driver.</li> </ul> </li> <li>Consider for response correlation.<ul> <li>Response correlation: 16-year-old males have twice the claim frequency as 16-year-old females, if frequency is our response/target.</li> </ul> </li> </ul>","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.5.classification/1.5.2.multivariate/#minimum-bias-procedures","title":"Minimum Bias Procedures","text":"<p>Assume there are two variables: Class and Territory.</p>","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.5.classification/1.5.2.multivariate/#steps","title":"Steps","text":"<ul> <li>Set up equations. Assume the class relativities to be variables (\\(c_{A}, c_{B}, c_{C}\\)) and territory relativities to be (\\(t_{1},t_{2},t_{3}\\)).</li> <li>Start with seed relativities for (\\(c_{A}, c_{B}, c_{C}\\)) and solve for (\\(t_{1},t_{2},t_{3}\\)).</li> <li>Then plug in the t-relativities to solve for c-relativities.</li> <li>Iterate until convergence.</li> <li>Rebase them.</li> </ul>","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.5.classification/1.5.2.multivariate/#advantages","title":"Advantages","text":"<ul> <li>Properly corrects exposure correlation.</li> </ul>","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.5.classification/1.5.2.multivariate/#disadvantages","title":"Disadvantages","text":"<ul> <li>Doesn't provide a way to test if the variable is statistically significant.</li> <li>Computationally inefficient (iterative method).</li> </ul>","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.5.classification/1.5.2.multivariate/#sequential-analysis","title":"Sequential Analysis","text":"<p>The only method allowed for California Personal Auto.</p>","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.5.classification/1.5.2.multivariate/#steps_1","title":"Steps","text":"<ul> <li>Perform Univariate analysis to obtain single relativities for a single variable.</li> <li>Use the Adjusted Pure Premium Approach to indicate relativities for a second variable.</li> <li>Do the same for the others, having adjusted for the exposures of the previous variable at each step.</li> </ul>","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.5.classification/1.5.2.multivariate/#advantages_1","title":"Advantages","text":"<ul> <li>Deals with exposure correlation.</li> </ul>","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.5.classification/1.5.2.multivariate/#disadvantages_1","title":"Disadvantages","text":"<ul> <li>Doesn't have a closed-form solution.<ul> <li>The solution varies with the order of the variables chosen for the analysis.</li> </ul> </li> </ul>","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.5.classification/1.5.3.special-classification/","title":"Special Classification","text":"","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.5.classification/1.5.3.special-classification/#territory-ratemaking","title":"Territory Ratemaking","text":"<ul> <li> <p>Rating Variables</p> <ul> <li>Can be Zip-codes, county level, or insurer-defined territory level.</li> <li> <p>Challenges</p> </li> <li> <p>Highly correlated with other variables.</p> </li> <li>Small areas, limited credibility (focus of this discussion).</li> <li> <p>Two Steps</p> </li> <li> <p>Establish Boundaries</p> <ul> <li>Considerations<ul> <li>Zip codes change over time.</li> <li>Counties are not granular enough to reflect geographic risk differences at a finer level of detail.</li> </ul> </li> </ul> </li> <li>Determine indicated rates for each territory<ul> <li>Preferably use GLMs due to correlation with other variables.</li> <li>Estimate Geographic systematic risk for each geographic unit.</li> <li>Distinguish it from random noise and systematic risk for other correlated non-geographic rating variables.<ul> <li>Prefer GLM.</li> </ul> </li> </ul> </li> <li> <p>GLMs (Generalized Linear Models)</p> </li> <li> <p>Data included</p> <ul> <li>Geo-physical data (e.g., Rainfall).</li> <li>Geo-demographic data (e.g., Population density).</li> </ul> </li> <li>Residual geographic variable: to account for unexplained geographic variation.</li> <li>Apply Spatial-smoothing techniques to this residual variable<ul> <li>Two approaches and consideration for over- and under-smoothing.</li> <li>Distance-based<ul> <li>Geographic unit's data is credibility weighted with data from other geographic units, with weights diminishing with distance.</li> <li>Advantages: Simple.</li> <li>Disadvantages:<ul> <li>Assumes that distance has the same impact for urban and rural risks.</li> <li>Don't consider physical boundaries (e.g., rivers, highways).</li> </ul> </li> <li>Suited for ==weather related perils==.</li> </ul> </li> <li>Adjacency-based<ul> <li>This data is credibility weighted with data from rings of surrounding geographic units, with weights diminishing with wider rings.</li> <li>Solves the two disadvantages above.</li> <li>Suited for ==socio-demographic perils== (theft).</li> </ul> </li> <li>Also take care of over-smoothing and under-smoothing.<ul> <li>Smoothing refers to reducing the weightage on the local data.</li> <li>Over: taking irrelevant data from distant locations.</li> <li>Under: giving local data too much credibility.</li> </ul> </li> </ul> </li> <li>The geographic unit levels can now be grouped into territories (clusters)<ul> <li>Clustering routines:</li> <li>Quantile methods<ul> <li>Clusters will have an equal number of observations or equal weights.</li> </ul> </li> <li>Similarity methods<ul> <li>Based on closeness of estimated relativities.</li> <li>Average Linkage similarity method<ul> <li>Tends to join clusters with smaller variances.</li> </ul> </li> <li>Centroid Similarity method<ul> <li>Tends to be more responsive to outliers.</li> </ul> </li> <li>Ward's clustering method<ul> <li>Tends to produce clusters with the same number of observations.</li> </ul> </li> </ul> </li> <li>Methods might result in non-contiguous territories unless a specific constraint is added to the method.</li> </ul> </li> </ul> </li> </ul>","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.5.classification/1.5.3.special-classification/#increased-limits","title":"Increased Limits","text":"<ul> <li> <p>Introduction</p> <ul> <li>Basic limit: Lowest available limit the insured can choose with a rating factor of 1.</li> <li>The correct relativity = ILF (Increased Limit Factor).</li> <li>Reasons<ul> <li>Personal wealth grows, more coverage required.</li> <li>\"Inflationary trends have more impact on increased limits than on the basic limit.\" (pdf) #doubt</li> <li>More lawsuits and higher jury awards over time.</li> </ul> </li> <li>Types of limits<ul> <li>Single limit.</li> <li>Compound limits: Two or more limits to each claim, e.g., per-claimant with a per-occurrence limit.</li> </ul> </li> <li>Why standard approaches are problematic?<ul> <li>Less data for higher limits: volatile results.</li> <li>Impractical to implement results from analysis, e.g., lower prices for higher limits.</li> </ul> </li> <li> <p>Standard ILF approach</p> </li> <li> <p>Rate at limit H = ILF(H) * Rate at basic Limit B.</p> </li> <li>#assumptions<ul> <li>All Underwriting (UW) expenses and profit are variable. Don't vary by limit.<ul> <li>In practice, profit loads for higher limits might be higher since they are more volatile.</li> </ul> </li> <li>Frequency and Severity are independent.</li> <li>Frequency is the same for all limits.<ul> <li>May not be true in practice, due to adverse or favorable selection.</li> <li>Favorable: If financially stable insureds have more assets to protect, they will buy higher limits.</li> <li>Adverse: Insureds expecting higher loss potential are more inclined to purchase higher limits.</li> </ul> </li> </ul> </li> <li>Limited Average Severity (LAS)<ul> <li>LAS(H) is the severity assuming every loss is capped at H (regardless of the actual policy limit).</li> <li>See also: where x is the size of Loss and f(x) is its distribution.</li> <li>Numerator = LAS(D) Limited Average Severity (LAS)</li> <li>Denominator = Expected value of x (Size of Loss)</li> </ul> </li> <li>Uncensored<ul> <li>Assume that we know the uncensored value of all claims.</li> </ul> </li> <li>Censored Losses<ul> <li>\"Note that it would be incorrect to use this same calculation to calculate LAS($250,000) or LAS($500,000) directly since we don\u2019t know what the losses would be from $100k or $250k policies ==if they had higher limits==, so it would be inappropriate to assume they would be unchanged if the limits on those policies were higher.\" (pdf)</li> </ul> </li> <li> <p>Other Considerations</p> </li> <li> <p>Historical data to calculate ILFs should be trended and developed to ultimate.</p> <ul> <li>Important since higher limits experience higher severity trends and the development can take longer for larger claims.</li> </ul> </li> <li>For deductible-truncated data, we can add back into the losses (for known claims). But we will still ==not know those from unknown claims that were fully below the deductible==.</li> <li> <p>Fitted Data Approach</p> </li> <li> <p>Fit curves to smooth out random fluctuations (resulting from limited data).</p> </li> <li>Common distribution choices<ul> <li>Lognormal</li> <li>Pareto</li> <li>Truncated Pareto</li> </ul> </li> </ul> </li> </ul>","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.5.classification/1.5.3.special-classification/#deductible-pricing","title":"Deductible Pricing","text":"<ul> <li> <p>Types of Deductibles</p> <ul> <li>Flat Dollar Deductibles<ul> <li>$250 applied to each claim.</li> </ul> </li> <li>Percentage Deductibles<ul> <li>Percentage applied to the coverage amount.</li> </ul> </li> <li> <p>Popularity reasons</p> </li> <li> <p>Reduce insured's premium.</p> </li> <li>Eliminate the need of handling small \"nuisance\" claims.</li> <li>Incentive for insured to avoid or mitigate loss.</li> <li>Reduce insured's catastrophic exposure.</li> <li>Excess Ratio(D) + Loss Elimination Ratio(D) = 1</li> </ul> </li> <li> <p>Discrete LER</p> <ul> <li>Indicated Deductible Relativity of $250 Deductible = Excess Ratio(D) = 1 - LER(D).</li> <li>If we know ground-up losses.</li> <li>\"A more common scenario in practice is that you don\u2019t know the true ground-up losses since you don\u2019t know about potential claims below the policy deductibles.\" (pdf)</li> <li>The deductible column shows the aggregate of policies who subscribed to no deductibles (\"None\"). They have net reported losses of $690,000. Had the deductible of $250 been applied to these policies, the net reported losses would have been $590,000 (claims below $250 are removed, and $250 from each claim larger than $250 is removed).</li> <li>Focus on $250 deductible policies. Net reported losses are the same as that assuming a deductible of $250 (because they are both the same).</li> <li>Now, look at $500 deductible policies. Its data is insufficient to tell us how many of such claims were above $250 and below $500. That data is lost, and thus the net reported losses assuming a $250 deductible is \"Unknown.\"</li> <li> <p>Continuous LER Formula</p> </li> <li> <p>where x is the size of Loss and f(x) is its distribution.</p> </li> <li>Numerator = LAS(D) Limited Average Severity (LAS)</li> <li>Denominator = Expected value of x (Size of Loss)</li> <li> <p>Other Considerations</p> </li> <li> <p>Loss Data should be trended and developed.</p> <ul> <li>Loss data for different deductibles can trend and develop at different rates.</li> </ul> </li> <li>GLM and Univariate analysis will consider the fact that \"claimant behavior will vary by deductible\" but LER doesn't.<ul> <li>An insured with a $1050 claim will not report if his deductible is $1000 given a small recovery and a fear of being surcharged on the next renewal.</li> </ul> </li> <li>\"favorable selection occurs in practice if low-risk insureds can choose higher deductibles. As a result, higher deductible policies are often more profitable. GLMs and univariate analysis will recognize this favorable selection, but the LER approach will not\" (pdf).</li> <li>Dollar saving implied by deductible relativity<ul> <li>Relativity of $250 deductible = 1.</li> <li>Relativity of $500 deductible = 0.90.</li> <li>For an insured (premium = $6000) moving from $250 to $500 deductible, the dollar credit = (1-0.90) * $6000 = $600.</li> </ul> </li> </ul> </li> </ul>","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.5.classification/1.5.3.special-classification/#fixed-expenses","title":"Fixed Expenses","text":"<ul> <li> <p>Components to recognize that fixed expense as a percentage of premium should decrease as the size of the policy increases.</p> <ul> <li>Small risks are not undercharged and large risks are not overcharged.</li> <li>Expense Constant<ul> <li>Flat dollar amount added to the premium of each risk.</li> </ul> </li> <li>Premium Discount</li> <li>Premium Discount</li> </ul> </li> <li> <p>Loss Constants</p> <ul> <li>Small work comp have worse loss experience.<ul> <li>Less sophisticated safety programs.</li> <li>Don't have return-to-work program for injured workers.</li> <li>Not impacted by/don't qualify for experience rating so no incentive to mitigate/prevent injuries.</li> </ul> </li> <li>One way to solve the above issue is to use GLMs to create new rating variables, varying the rate by size of risk.<ul> <li>In use by some of the current companies.</li> </ul> </li> <li>The more traditional way is to use Loss Constants.</li> <li><code>63000/(75000 + 100C) = 144000/200000 =&gt; C = \\$125</code></li> <li>or</li> <li><code>63000/(75000 + 100C) = 144000/(200000 + 50C) =&gt; C = \\$160</code></li> </ul> </li> </ul>","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.5.classification/1.5.3.special-classification/#insurance-to-value","title":"Insurance to Value","text":"<p>If underinsured to value (ITV).</p> <ul> <li> <p>\"In general, the indicated rate per $1k of coverage will be higher for underinsured homes as long as partial losses are possible (i.e., not all losses are total losses), which is generally true in practice.\" (pdf)</p> </li> <li> <p>Two issues when insured less than the replacement cost:</p> <ul> <li>Not fully covered in an event of a total or near-total loss.</li> <li>If the insurer assumes full coverage (to value), the premium charged for underinsured policies will not be adequate to cover expected losses.<ul> <li>If an insured doesn't insure his house to its total replacement cost (e.g., House 3 has a replacement cost of $250k but is only insured to $200k), then the total expected severity of House 3 will be higher (compared to if it were insured to its full replacement cost). In such a case, the <code>Indicated Rate per \\$1k</code> for Home 3 will be $6 (higher than $5, which is for those houses insured to full value). ==Thus, if we were to charge Home 3 the same $5 as the other houses, we would be undercharging Home 3==.</li> </ul> </li> <li>The insurer might increase the premium over time to cover the expected losses (overall), leading to adequate premiums. But in such a case, the fully insured homes will be subsidizing under-insured homes.</li> </ul> </li> <li> <p>\"the dollar change in the indicated rate per $1k of coverage as the ITV increases depends on the severity distribution\" (pdf).</p> <ul> <li>Left-skewed, uniform and right-skewed.</li> <li> <p>ITV Initiatives</p> </li> <li> <p>GRC (Guaranteed Replacement Cost)</p> <ul> <li>The coverage that will cover for the actual replacement cost of the house (i.e., pay for the total loss) if the house was fully insured. But this was discontinued while observing a few catastrophes in the 1990s, where the insurers had to pay more in losses than anticipated.</li> </ul> </li> <li>Better estimation of Actual cost</li> <li> <p>Coinsurance</p> </li> <li> <p>If the coinsurance requirement is 80% ITV, then a home should be insured for at least 80% of the ITV. If underinsured then a penalty is applied.</p> </li> <li>\"coinsurance apportionment ratio\" (pdf)<ul> <li><code>a = min[coverage amount / (required coinsurance % * replacement cost),1]</code></li> </ul> </li> <li>Indemnity payment before application of deductible: <code>I = min[a * Loss amount, coverage amount]</code></li> <li>Coinsurance penalty: <code>e = min[Loss amount, coverage amount] - I</code></li> <li>Example: suppose the replacement cost for a property is $500,000, the coinsurance requirement is 80% ITV, the selected coverage amount is $300,000, and the deductible is $1,000.<ul> <li>Coinsurance apportionment ratio, a = min[30000/(50000*80%),1] = min[3/4,1] = 0.75.</li> <li>Indemnity payment before deductible, I = 0.75 * $200000 loss = $150000.</li> <li>Payment after deductible = $150000 - $1000 = $149000.</li> <li>Coinsurance Penalty e = min[200000, 300000] - $150000 = $50000.</li> </ul> </li> </ul> </li> </ul>","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.7.indications-vs-reality/","title":"Indications vs Reality","text":"","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.7.indications-vs-reality/1.7.1.credibility/","title":"Credibility","text":"<ul> <li> <p>1 Backlink</p> <ul> <li>Use CSLC to lookup the credibility, MSL and EER from the ISO tables<ul> <li>Looking up the tables is not part of Exam 5. But we may be asked to calculate the credibility or calculate the EER given the numerator and denominator.</li> </ul> </li> <li> <p>3 necessary criteria</p> </li> <li> <p>0\u2264Z\u22641</p> </li> <li>Z should increase as n increases (dZ/dn\u22650).</li> <li>Z should increase as n increases at a decreasing rate (d/dn(Z/n)&lt;0).</li> <li> <p>Methods</p> </li> <li> <p>Classical</p> <ul> <li>Z\u00d7Observed+(1\u2212Z)\u00d7Related\u00a0Experience</li> <li>Partial<ul> <li>Claim counts (n): Z=min(n/N0\u200b\u200b,1)</li> <li>Exposures: Z=min(e/(N0\u200b/F)\u200b,1)<ul> <li>Exposures for full credibility = N0\u200b/expected\u00a0frequency(F).</li> </ul> </li> </ul> </li> </ul> </li> <li>Buhlmann<ul> <li>Least Squares Credibility.</li> <li>Estimate = Z\u00d7(Observed\u00a0Experience)+(1\u2212Z)\u00d7Prior\u00a0Mean</li> <li>And Z=N/(N+K)<ul> <li>K=EVPV\u00a0/\u00a0VHM<ul> <li>Expected Value of Process Variance, EVPV = E[Var(X)].</li> <li>Variance of Hypothetical Means, VHM = Var(E[X]).</li> </ul> </li> </ul> </li> </ul> </li> <li>Bayesian Analysis</li> </ul> </li> </ul>","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.7.indications-vs-reality/1.7.1.credibility/#complement-of-credibility","title":"Complement of Credibility","text":"","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.7.indications-vs-reality/1.7.1.credibility/#properties","title":"Properties","text":"<ul> <li>Accurate (close to target).</li> <li>Unbiased (on target on average).</li> <li>Statistically independent from base statistic.</li> <li>Available (else not practical).</li> <li>Easy to compute (else difficult to justify).</li> <li>Logical relationship to base statistic (\" \" \" justify).</li> </ul>","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.7.indications-vs-reality/1.7.1.credibility/#complements-in-first-dollar-ratemaking","title":"Complements in First Dollar Ratemaking","text":"<p>Ground up losses (or small deductibles): Loss Cost (LC).</p> <ul> <li>LC of a Larger group including subject group<ul> <li>Regional or Countrywide for state data.</li> <li>May be biased.<ul> <li>There is a reason why the subject group has been separated from the larger group.</li> </ul> </li> </ul> </li> <li>LC of a larger related group<ul> <li>Usually biased.</li> <li>Logical relationship if chosen reasonably.</li> </ul> </li> <li>Rate change from large group applied to present rates<ul> <li>Adjusted version of prior complement to reduce bias.</li> <li>Complement = Current LC (Subject) * (Large group Ind LC / Large group Current Avg LC).</li> </ul> </li> <li>Harwayne's method<ul> <li>We are trying to find the complement of credibility for LC (Pure premium) of class A.</li> <li>Calculate A's, B's, C's average pure premium by using Class A's exposures as the weight.</li> <li>Calculate adjustment factor (Class B / Class A avg premium).</li> <li>Multiply PP for each class by adjustment factor.</li> <li>Exposure weight the PP of the other classes to get the complement.</li> </ul> </li> <li>Trended Present rates #todo</li> <li>Competitor's rates<ul> <li>Biased, inaccurate, and difficult to obtain.</li> </ul> </li> </ul>","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.7.indications-vs-reality/1.7.2.implementation/","title":"Implementation","text":"<p>Off-balance factor.</p>","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.7.indications-vs-reality/1.7.2.implementation/#calculating-new-rates-for-an-existing-product-steps","title":"Calculating New Rates for an Existing Product (Steps)","text":"<p>This section focuses on the ==last two==:</p> <ul> <li>Select Target Overall Average Premium.</li> <li>Finalize the rating algorithm.</li> <li>Select final new rates for each rating variable.</li> <li>==Select final additive amounts (e.g., expense fees) if applicable==.</li> <li>==Derive final base rate to achieve target premium==.</li> </ul>","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.7.indications-vs-reality/1.7.2.implementation/#expense-fees","title":"Expense Fees","text":"<ul> <li>Fixed expense Fee per exposure = <code>Fixed Expense per exposure / (1 - V - Q)</code>.<ul> <li>Can be converted into a per-policy basis by multiplying by the average number of exposures per policy.</li> </ul> </li> </ul>","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.7.indications-vs-reality/1.7.2.implementation/#deriving-new-base-rate","title":"Deriving new Base Rate","text":"","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.7.indications-vs-reality/1.7.2.implementation/#tip-dont-read-the-theory-just-go-for-the-examples-to-learnunderstand-do-it-in-excel-as-all-of-them-use-the-same-base-example-to-demonstrate-the-process","title":"tip Don't read the theory, just go for the examples to learn/understand, do it in Excel as all of them use the same base example to demonstrate the process.","text":"<ul> <li> <p>Without Rating Factor Changes</p> <ul> <li>Only change the Base Rate.</li> <li>An additive component may also change (but this usually doesn't happen, e.g., a fixed expense fee wouldn't change with a rate change).</li> <li> <p>With Rating Factor Changes</p> </li> <li> <p>Problem used in the examples below.</p> </li> <li>Extension of Exposures<ul> <li>Example (pdf).</li> </ul> </li> <li>Approximated Average Rate Differential<ul> <li>EXACT Average Rate Differential (pdf).</li> <li>Approximated Average Rate Differential (pdf).</li> </ul> </li> <li>Approximated Change in Average Rate Differential<ul> <li>EXACT Change in Average Rate Differential (pdf).</li> <li>Approximated Change in Average Rate Differential (pdf).</li> <li>See also: Approximated Change in Average Rate Differential<ul> <li>Best: Variable premium at base level.</li> <li>Best (=): adjusted exposures as in Adjusted Pure Premium Method.</li> <li>Second-best: Using exposures.<ul> <li>But less accurate in theory. Only do if data for above 2 is unavailable.</li> </ul> </li> <li>==Never weight with Actual Premiums NOT at BASE Level== to avoid double counting of the relativities.</li> <li>Off-balance = Current weighted-relativity / Proposed weighted-relativity.</li> </ul> </li> </ul> </li> <li> <p>Rules for Weighting</p> </li> <li> <p>Approximated Change in Average Rate Differential.</p> <ul> <li>Best: Variable premium at base level.</li> <li>Best (=): adjusted exposures as in Adjusted Pure Premium Method.</li> <li>Second-best: Using exposures.<ul> <li>But less accurate in theory. Only do if data for above 2 is unavailable.</li> </ul> </li> <li>==Never weight with Actual Premiums NOT at BASE Level== to avoid double counting of the relativities.</li> <li>Off-balance = Current weighted-relativity / Proposed weighted-relativity.</li> </ul> </li> <li> <p>Minimum Premiums</p> </li> <li> <p>Ensure that premium will cover expected fixed expenses and some minimal amount of expected losses.</p> </li> <li>Effect = <code>Premium with minimum</code> / <code>Premium without (with old) minimum</code>.</li> <li>Base Rate offset = 1 / (1+Effect).</li> <li> <p>Rate Capping for One Variable</p> </li> <li> <p>\"If we want to cap the impact of a rate change for a single variable, we will need to make up any premium loss resulting from the cap on other levels of the variable to obtain the same overall target premium\" (pdf).</p> </li> <li>If we cap a non-base level<ul> <li>01 Textbook approach.</li> <li>02 Allocate the shortfall directly and back into rels (pdf).</li> <li>03 Figure out an equation.</li> </ul> </li> <li>If we cap a Base Level<ul> <li>01 Textbook approach.</li> <li>02 Allocate the shortfall directly and back into rels (pdf).</li> <li>03 Figure out an equation.</li> </ul> </li> <li>Finish Rate Capping for One Variable by 6:30 PM on March 22, Saturday.</li> <li> <p>Note</p> </li> <li> <p>Off-Balance Factor and the Rationale behind</p> <ul> <li>Premium = Base Rate * Combined Variables Multiplicative Rating Factors + Additive Fee.</li> <li>Average Premium = Base Rate * Exposure-weighted Average Rating Factor + Additive Fee.</li> <li>If the Exposure-weighted Average RF term cancels out (in case there are no rating factor changes), we are done.<ul> <li>The E-w Avg RF term is shown as the last terms in the numerator and denominator of the fraction in the image above.</li> </ul> </li> <li>If they don't cancel out, we will have to multiply by the Off-balance factor.</li> </ul> </li> <li>For improved approximation we use Current Variable Premium at Base: \"\"Variable\" means we are only considering the variable portion of premium (i.e., not including the additive fees), and \"at base\" means we don\u2019t include any relativities for the variable we are analyzing (i.e., all levels of the variable are rated the same as the base level with a factor of 1).\" (pdf)</li> </ul> </li> </ul>","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.7.indications-vs-reality/1.7.2.implementation/#premium-transition-rules","title":"Premium Transition Rules","text":"<p>To ensure that no individual risk goes up more than a certain percentage.</p> <ul> <li>Rules that limit the change that insureds can see on their premium on renewal<ul> <li>20% cap = (1+15%)(1+4.35%).</li> </ul> </li> <li>Considerations<ul> <li>What's the cap amount, and what does it do to your total profit?<ul> <li>Based on total profit scenario testing.</li> </ul> </li> <li>Fully allow to reflect non-rate large changes (e.g., change in exposures, claim surcharges). But ONLY cap the rate.</li> <li>Transition periods should be short.<ul> <li>Since multiple transition rules can start to overlap.</li> </ul> </li> <li>Decision of implementation<ul> <li>If uncapped impact is 5% but the impact of the first year is 3.5%, should we raise the base rates to 5% in the first year itself, or wait for future renewals for the 5% to be realized?</li> </ul> </li> </ul> </li> </ul>","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.7.indications-vs-reality/1.7.2.implementation/#expected-distribution","title":"Expected Distribution","text":"<ul> <li>Rate changes are measured by the in-force on-level premiums.</li> <li>They don't consider the effect of the change to the mix-of-business.</li> <li>Optimized Pricing techniques consider the impact on mix, the propensity to renew after a rate change.</li> </ul>","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.7.indications-vs-reality/1.7.2.implementation/#using-bureau-or-competitor-rate","title":"Using Bureau or Competitor Rate","text":"<ul> <li>How to make B/C rates more applicable<ul> <li>Adjust for fixed expense differences<ul> <li>Adjust competitor expense fee by a factor = Your Fixed Expense exposure / Competitor Fixed Expense per exposure.</li> <li>An approximation of their fixed expense using our fixed expenses.</li> </ul> </li> <li>Adjust for Variable expenses<ul> <li>Adjust competitor base rate or expense fee by a factor = (Competitor variable PLR / Your Variable PLR).</li> <li>Competitor PLR in the numerator because variable PLR go in the denominator of the loss ratio formula for rate change.</li> <li>PLR: Permissible Loss Ratio.</li> </ul> </li> <li>Adjust for loss costs<ul> <li>Factor = Your Expected Loss Cost / Competitor's.</li> </ul> </li> <li>Class differences can be accounted for<ul> <li>Using actuarial judgment.</li> </ul> </li> </ul> </li> </ul>","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.7.indications-vs-reality/1.7.2.implementation/#communicating-and-monitoring","title":"Communicating and Monitoring","text":"<ul> <li>Communicate impact to stakeholders.</li> <li>Explain ratemaking assumptions to regulators.</li> <li>The bigger the rate change, the more questions we get from regulators and internal folks.</li> </ul>","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.7.indications-vs-reality/1.7.3.other-considerations/","title":"Other Considerations","text":"","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.7.indications-vs-reality/1.7.3.other-considerations/#regulatory-constraints","title":"Regulatory Constraints","text":"<ul> <li>Examples of Regulations in the U.S.<ul> <li>Limiting the amount of rate changes:<ul> <li>Overall Book.</li> <li>Individual.</li> </ul> </li> <li>Require ==written notice to insureds== about large rate increases.</li> <li>Prohibit certain rating variables.<ul> <li>e.g., Credit score (not related to direct behavior and controllable).</li> </ul> </li> <li>Prescribing certain ==Ratemaking Techniques==.<ul> <li>e.g., Multivariate Techniques.</li> </ul> </li> <li>Revise ratemaking assumptions.<ul> <li>e.g., disagreement with regulator on the choice of loss trend.</li> </ul> </li> </ul> </li> <li>Actions<ul> <li>Take legal action.</li> <li>Revise ==UW guidelines== to limit business at inadequate rates.</li> <li>Revise ==marketing== to limit business at inadequate rates.</li> <li>Use a proxy variable when a variable is prohibited.</li> </ul> </li> </ul>","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.7.indications-vs-reality/1.7.3.other-considerations/#operational-constraints","title":"Operational Constraints","text":"<ul> <li>Examples of Operational Constraints<ul> <li>System limitations<ul> <li>Depends on:<ul> <li>Cost to implement change (extent of rate change: new rate or rating variable).</li> <li>Number of computer systems impacted (policy processing, claims).</li> </ul> </li> </ul> </li> <li>Resource constraints<ul> <li>Example: New rating variable requiring specialized staff / inspectors, which might be ==very expensive relative to the expected benefit of the additional information==.<ul> <li>Collect and process data for the new variable.</li> <li>Inspectors to visually inspect the insured property.</li> </ul> </li> <li>Cost-benefit analysis<ul> <li>Increased profits from implementation exceeds the cost implementation.</li> <li>Requires assumptions about the indirect effects of the change.</li> </ul> </li> </ul> </li> </ul> </li> </ul>","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.7.indications-vs-reality/1.7.3.other-considerations/#marketing-constraints","title":"Marketing Constraints","text":"<ul> <li> <p>How much are the insureds willing to pay for insurance?</p> <ul> <li>This determines: How many insureds buy and how much total profit can be earned?</li> <li> <p>Factors influencing insured's purchasing decision</p> </li> <li> <p>Competitor's prices.</p> </li> <li>Overall cost of the product.</li> <li>\"Rate changes (for existing customers)\" (pdf).</li> <li>Insured characteristics.<ul> <li>How sensitive are they to prices?</li> </ul> </li> <li>Customer ==satisfaction== and brand ==loyalty==.</li> <li> <p>Traditional Techniques for Incorporating Mkt. Considerations</p> </li> <li> <p>Things to consider: #checklist</p> </li> <li>Competitive Comparison<ul> <li>Obtain rates for a competitor: re-rate existing policies with competitor's rates and insurer's proposed rates and see which one is cheaper. Check the market segments for which this rate will be cheaper.</li> <li>Difficult to obtain competitors rates.</li> <li>The UW Guidelines of the competitors are unknown so ==unfair comparison==.</li> <li>Metrics<ul> <li>% Competitive Position = Competitor Premium / Company Premium - 1.<ul> <li>How better is the competitor than you? Positive is bad for you.</li> </ul> </li> <li>$ Competitive Position = Competitor Premium - Company Premium.<ul> <li>Same... just additive.</li> </ul> </li> <li>% Win = # of risks meeting criteria / total # of risks.<ul> <li>Criteria e.g.: Premium is lower than Competitor. How many risks have success in terms of competitive advantage?</li> </ul> </li> <li>Rank = Rank of the Company premium when compared to others.</li> </ul> </li> </ul> </li> <li>Close Ratios<ul> <li>Monitor these ratios closely after the implementation to identify impact on new and renewal customers.</li> <li>Close Ratio = # of Accepted Quotes / # of Quotes.</li> <li>Retention Ratio = # of Policies Renewed / # of ==Potential Renewal== Policies.</li> <li>% Policy Growth = Delta # of Policies during the period / # of Policies at the beginning of the period.</li> </ul> </li> <li>Distributional Analysis<ul> <li>Identify which segments of the book of business are growing and shrinking over time.</li> <li>Examples<ul> <li>20% of new business: aged 20-25 ==vs== 10% of existing business: aged 20-25.</li> <li>If 30% of the drivers in a state are aged 20-25, and the insurer insures only 10% of those drivers ==then== the company is under-represented in that segment (20-25) of the market.</li> </ul> </li> </ul> </li> <li>\"Dislocation analysis (also called disruption analysis)\" (pdf).<ul> <li>Impact of rate changes on existing customers.</li> <li>Quantify how many customers will receive the rate changes of different %s and $ amounts.<ul> <li>Because customers with large changes may have a lower probability of retention.</li> </ul> </li> <li>This way the company can try and take proactive action to prepare the customer for the rate change.<ul> <li>e.g., discuss the possibility of increasing the deductible to help offset the rate increase.</li> </ul> </li> </ul> </li> <li> <p>Systematic Techniques for Incorporating Mkt. Considerations</p> </li> <li> <p>Advanced Techniques</p> <ul> <li>Examples</li> <li>Lifetime Value Analysis<ul> <li>Also known as Asset Share Pricing.</li> <li>Look at profitability of an insured over the entire lifetime with the insurer (in contrast to the short-term time horizon with the traditional approaches).</li> <li>Assumptions<ul> <li>Insured's probabilities of renewal.</li> <li>Expected profitability over lifetime.</li> </ul> </li> </ul> </li> <li>Optimized Pricing<ul> <li>Multivariate techniques to determine Price Elasticity.<ul> <li>Of the new and renewal customers based on their characteristics.</li> </ul> </li> <li>New customers are more price sensitive.<ul> <li>Takes more effort for existing customers to shop for insurance and change carriers.</li> </ul> </li> <li>Optimized Pricing techniques consider the impact on mix, the propensity to renew after a rate change.</li> </ul> </li> </ul> </li> <li>\"With these estimates in addition to the loss estimates, the insurer can test different scenarios of rate changes and how they will impact the total profit and growth of the insurer.\" (pdf).</li> </ul> </li> </ul>","tags":["Five"]},{"location":"exam-5/1.ratemaking/1.7.indications-vs-reality/1.7.3.other-considerations/#underwriting-cycles","title":"Underwriting Cycles","text":"<p>The profit and growth of the entire insurance industry tends to be cyclical in nature.</p> <ul> <li>A cycle between \"Hard\" and \"Soft\" markets.<ul> <li>Hard: prices are high and insurer profits are high, but growth is low due to high prices.</li> <li>Soft: prices are low and companies are growing.</li> </ul> </li> <li>Cycle<ul> <li>Arbitrarily starting from the hard market.</li> <li>Market is ==hard==.</li> <li>To attract more business, some insurers lower rates.</li> <li>To maintain competitiveness, all insurers lower rates. Market becomes ==soft==.</li> <li>In response to low (or negative) profit levels, insurers restrict business or increase prices to improve profitability.</li> <li>Eventually, all insurers raise prices. Market becomes ==hard==.</li> </ul> </li> </ul>","tags":["Five"]},{"location":"exam-5/2.reserving/","title":"Reserving","text":"","tags":["Five"]},{"location":"exam-5/2.reserving/2.0.introduction/","title":"Introduction","text":"<p>Once you have finished ratemaking, you will know how the Insurance product is priced by looking at the past year losses. But now we are waiting for insureds with their claims. Unfortunately, we don't know when they will get into accidents (God Forbid!) and when they will file their claims. Thus, what we need to do is to set up a system by which we have a good estimate about how much to keep aside for paying off the claims.</p> <p>Wait a minute. If we had collected premiums from them, then we definitely have enough rates to pay them off. Why then do we need to estimate how much to keep aside from them. And if so, what are we doing with the money not kept aside?</p> <p>Investments!</p> <p>If you remember, we mentioned that the profit margins are not the only way the insurer gets to earn profit.</p>","tags":["Five"]},{"location":"exam-5/2.reserving/2.0.introduction/#goals-of-reserving","title":"Goals of Reserving","text":"","tags":["Five"]},{"location":"exam-5/2.reserving/2.0.introduction/#why-is-it-important-to-accurately-estimate-unpaid-claims","title":"Why is it important to accurately estimate unpaid claims?","text":"<ul> <li>Internal management<ul> <li>Make business decisions in Underwriting (UW) and pricing.</li> <li>Inaccurately high estimates can lead to raising rates, tightening UW guidelines.</li> </ul> </li> <li>Investors<ul> <li>Profitability of insurer affects returns paid to investors.</li> <li>Inaccurately high estimates lower the insurer's profits, making it a worse investment to potential investors.</li> </ul> </li> <li>Regulators<ul> <li>Monitor insurer solvency.</li> <li>Inaccurately high estimates cause low profit, leading to intervention and restriction of the insurer's ability to write new business.</li> </ul> </li> </ul>","tags":["Five"]},{"location":"exam-5/2.reserving/2.0.introduction/#responsibility-appointed-actuary-aa","title":"Responsibility Appointed Actuary (AA)","text":"<ul> <li>In the U.S., Property &amp; Casualty (P&amp;C) insurers must obtain a Statement of Actuarial Opinion signed by an AA.<ul> <li>It's a financial reporting requirement. AA also required in other countries: Canada, Australia, Slovenia.</li> </ul> </li> </ul>","tags":["Five"]},{"location":"exam-5/2.reserving/2.0.introduction/#terminology","title":"Terminology","text":"<ul> <li>Accounting Date<ul> <li>Group of claims being analyzed. Paid/unpaid split. e.g., Unpaid amount as of 12/31/2013.</li> </ul> </li> <li>Valuation Date (&lt;=&gt; Accounting Date)<ul> <li>Transactions included in the analysis. e.g., Data through 3/31/2014 to estimate unpaid claims as of 12/31/2013.</li> </ul> </li> <li>Review Date (&gt; Valuation Date)<ul> <li>Material information known to the actuary is included in the analysis.</li> </ul> </li> <li>Development<ul> <li>Changes in values between Valuation Dates for a given Accounting Date.</li> </ul> </li> <li>Estimated claims reserve<ul> <li>Estimated unpaid claims.</li> </ul> </li> <li>Carried claims reserve (aka Booked Reserves)<ul> <li>The reserve value that actually appears on the insurer's financial statements.</li> </ul> </li> </ul>","tags":["Five"]},{"location":"exam-5/2.reserving/2.0.introduction/#components-of-unpaid-claims","title":"Components of Unpaid Claims","text":"<p>Ultimate claims = Paid claims + Unpaid claims.</p> <ul> <li>1 Case Reserves</li> <li>2 Provision for development on known claims</li> <li>3 Reopened claims reserve</li> <li>4 \"Pure\" IBNR</li> <li>5 Provision for claims in transit<ul> <li>Incurred and reported but not recorded.</li> </ul> </li> <li>IBNR = #4 + #5</li> <li>IBNER = #2 + #3<ul> <li>Some insurers consider #3 as new claims, in which case it becomes part of \"pure\" IBNR.</li> </ul> </li> <li>Broad Def of IBNR = #2 + #3 + #4 + #5</li> <li>Ultimate claims = Paid + Case Reserves + IBNR (Broad definition of IBNR).</li> <li>Ultimate claims = Paid + Case Reserves + IBNR Reserve + IBNER Reserve.</li> </ul>","tags":["Five"]},{"location":"exam-5/2.reserving/2.0.introduction/#asop-43","title":"ASOP 43","text":"<p>About the ==estimation==.</p> <ul> <li>Consider the intended use of the multiple purposes.<ul> <li>Check if they have conflicting outcomes.</li> </ul> </li> <li>Limited Data &amp; Resources constraints should be documented and communicated.</li> <li>Regarding an ==estimate==, considerations:<ul> <li>Identify the intended measure.<ul> <li>\"High estimate\" or \"Mean estimate\"... ==NOT \"Best\" or \"Actuarial\" estimate==.</li> </ul> </li> <li>Note if any amounts are discounted for TVM.</li> <li>For specified recoverables.<ul> <li>Is the estimate gross or net?</li> <li>Extent of collectability when estimates are affected.</li> </ul> </li> <li>Types of Loss Adjustment Expense (LAE) included in estimate.</li> <li>What claims does this estimate cover?</li> <li>Are there any other items needed to describe the scope sufficiently? #doubt</li> <li>Discard any immaterial items (out of scope).<ul> <li>But, document those items.</li> </ul> </li> <li>Be familiar with the nature of the claims being estimated. Be familiar with the Line of Business (LOB).<ul> <li>Don't mechanically apply a method on the data without understanding what exactly is happening.</li> <li>Coverage conditions impacting Frequency &amp; Severity.</li> <li>Claim Adjustment Process.</li> <li>Potential Recoverables.</li> </ul> </li> </ul> </li> <li>While performing ==analysis==, considerations:<ul> <li>Appropriate method and model for producing estimates.</li> <li>Different methods for different components of the analysis.</li> <li>Should you rely upon a single method?</li> <li>What methods were used in prior analysis?</li> <li>Sensitivity analysis: Reasonable alternate assumptions.</li> <li>If there are interactions between different sources of recoverables.</li> <li>Should estimates be net/gross/both of recoverables?</li> <li>Impact of external (economic/regulatory) conditions?<ul> <li>A law change that is being debated, and if it's passed, it affects the insurer's rates.</li> </ul> </li> <li>Any operational changes?<ul> <li>That might impact the estimate.</li> </ul> </li> <li>Uncertainty associated with the analysis and estimate.<ul> <li>Point vs Interval estimates. Are we in the ballpark?</li> </ul> </li> </ul> </li> <li>Reasonability check of estimates.<ul> <li>Sanity check.</li> </ul> </li> <li>Presentation of the estimate, depends on intended purpose.<ul> <li>(e.g., point vs range).</li> </ul> </li> <li>Document and communicate.<ul> <li>Intended purpose.</li> <li>Scope.</li> <li>Significant limitations.</li> <li>Important dates.</li> <li>Significant risks/uncertainties.</li> <li>Significant events.</li> <li>Assumptions.</li> </ul> </li> </ul>","tags":["Five"]},{"location":"exam-5/2.reserving/2.2.berquist-sherman/","title":"Berquist-Sherman","text":"","tags":["Five"]},{"location":"exam-5/2.reserving/2.3.evaluation-of-techniques/","title":"Evaluation of Techniques","text":"","tags":["Five"]},{"location":"exam-5/2.reserving/2.1.development-techniques/","title":"Development Techniques","text":"","tags":["Five"]},{"location":"exam-5/2.reserving/2.1.development-techniques/2.1.1.chain-ladder-dev/","title":"Chain Ladder / Dev","text":"","tags":["Five"]},{"location":"exam-5/2.reserving/2.1.development-techniques/2.1.1.chain-ladder-dev/#obtain-the-cumulative-triangle","title":"Obtain the Cumulative Triangle","text":"<p>In the above example, we already have the cumulative paid triangle. If we were given incremental paid, we would have to derive the cumulative paid triangles.</p>","tags":["Five"]},{"location":"exam-5/2.reserving/2.1.development-techniques/2.1.1.chain-ladder-dev/#create-an-age-to-age-ldf-triangle","title":"Create an age-to-age LDF triangle","text":"<p>Just divide the \\(n+1\\) term maturity by the \\(n\\) term maturity (e.g. obtain the 12-24 maturity for 2016 by</p> \\[ \\dfrac{\\text{AY 2016 24 month Cumulative paid}}{\\text{AY 2016 12 month Cumulative paid}} \\] <p>Do the same for all possible maturities and years</p> <p>Info</p> <p>For 2017 its not possible, because only 12-month maturity available</p>","tags":["Five"]},{"location":"exam-5/2.reserving/2.1.development-techniques/2.1.1.chain-ladder-dev/#use-aj-to-select-the-ldfs-for-each-maturity","title":"Use #aj to select the LDFs for each Maturity","text":"<p>It is important to justify your selections, where ever you do them.</p> <p>Usually you can just take an average, but if you find some patterns that you wish to take into account, you should do that (for example, by taking a latest 3 year average if the previous year show some *weird* values)</p>","tags":["Five"]},{"location":"exam-5/2.reserving/2.1.development-techniques/2.1.1.chain-ladder-dev/#compute-the-cdfs","title":"Compute the CDFs","text":"<p>Just the Cumulative Development factor, to make finding the ultimate values more convenient.</p> <p>Simply just product all the LDFs to get the <code>12-ultimate</code> CDF. If you do that for LDFs from 24-36, 36-48 and so on... you will get the <code>24-ult</code> CDF.</p> <p>Success</p> <p>Later, you will find out that the CDFs are very useful in development techniques like Bornhuetter Ferguson, Cape Cod, Case Outstanding and also when we are doing some retroactive testing in the Evaluation section.</p>","tags":["Five"]},{"location":"exam-5/2.reserving/2.1.development-techniques/2.1.1.chain-ladder-dev/#use-cdfs-to-develop-the-year-of-interest-using-the-latest-maturity-value","title":"Use CDFs to develop the year of interest using the latest maturity value.","text":"<p>Viola, just multiply.</p>","tags":["Five"]},{"location":"exam-5/2.reserving/2.1.development-techniques/2.1.2.expected-claims/","title":"Expected Claims","text":"","tags":["Five"]},{"location":"exam-5/2.reserving/2.1.development-techniques/2.1.3.bornhuetter-ferguson/","title":"Bornhuetter-Ferguson","text":"","tags":["Five"]},{"location":"exam-5/2.reserving/2.1.development-techniques/2.1.4.cape-cod/","title":"Cape Cod","text":"","tags":["Five"]},{"location":"exam-5/2.reserving/2.1.development-techniques/2.1.5.frequency-severity/","title":"Frequency-Severity","text":"","tags":["Five"]},{"location":"exam-5/2.reserving/2.1.development-techniques/2.1.6.case-outstanding/","title":"Case Outstanding","text":"","tags":["Five"]},{"location":"exam-5/3.the-delta/3.2.estimating-alae/","title":"Estimating ALAE","text":"","tags":["Five"]},{"location":"exam-5/3.the-delta/3.3.estimating-ulae/","title":"Estimating ULAE","text":"","tags":["Five"]},{"location":"exam-5/3.the-delta/3.1.recoveries/","title":"Recoveries","text":"","tags":["Five"]},{"location":"exam-5/3.the-delta/3.1.recoveries/3.1.1.salvage-and-subrogation/","title":"Salvage &amp; Subrogation","text":"","tags":["Five"]},{"location":"exam-5/3.the-delta/3.1.recoveries/3.1.2.reinsurance/","title":"Reinsurance","text":"","tags":["Five"]},{"location":"exam-7/","title":"Advanced Estimation of Claims Liabilities","text":"","tags":["Seven","Exam"]},{"location":"exam-7/chain-ladder-variance-calc/","title":"Chain Ladder Reserve Variance Estimation","text":"","tags":["Seven","Exam","Technique"]},{"location":"exam-7/chain-ladder-variance-calc/#steps","title":"Steps","text":"<ul> <li> Find the age-to-age factors and \\(LDF_{k}\\) (select simple/volume-weighted/\\(C_{jk}^2\\) weighted)</li> <li> \"Square\" the Cumulative Paid Losses (Blue Box)</li> <li> Calculate the variance proportionality factor \\(\\alpha_{k}^2\\)</li> <li> Form the \\(MSE\\) table. (Use muscle memory)</li> <li> Find the \\(MSE_{AY}\\) by summing the rows. (Answer)</li> </ul>","tags":["Seven","Exam","Technique"]},{"location":"blog/archive/2025/","title":"2025","text":""},{"location":"blog/archive/2023/","title":"2023","text":""},{"location":"blog/category/essentials/","title":"Essentials","text":""},{"location":"blog/category/gotchas/","title":"Gotchas","text":""},{"location":"blog/category/statistics/","title":"Statistics","text":""},{"location":"blog/category/tutorials/","title":"Tutorials","text":""},{"location":"blog/category/motivation/","title":"Motivation","text":""},{"location":"blog/category/thought-experiment/","title":"Thought Experiment","text":""},{"location":"blog/page/2/","title":"Writings on the Wall","text":""},{"location":"blog/archive/2025/page/2/","title":"2025","text":""}]}